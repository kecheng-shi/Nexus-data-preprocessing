{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1387d3c3",
   "metadata": {},
   "source": [
    "# Nexus Data Preprocessing\n",
    "\n",
    "This notebook organises the equity time-series pipeline with clearly defined stages and function-level context inspired by the \"Advanced Data Preprocessing Techniques for Financial\" framework."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57aa47",
   "metadata": {},
   "source": [
    "## Define: Preprocessing Structure\n",
    "- **Define: Data Acquisition** – `preprocess_cat_excel` and `preprocess_cat_excel_simple` load Excel sources, infer canonical financial fields, and order the timeline.\n",
    "- **Define: Data Normalisation** – `_first_present`, `_coerce_datetime`, `adjust_ohlc_with_adjclose`, and `winsorize` align schemas, cleanse timestamps, and stabilise price inputs.\n",
    "- **Define: Feature Engineering** – `compute_rsi`, `compute_macd`, `compute_bbands`, and the rolling volatility steps convert raw prices into momentum and dispersion signals.\n",
    "- **Define: Output & Interfaces** – `main`, the public alias, and helper shims expose the pipeline for CLI execution and reusable library calls.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea2df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from typing import Optional, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867c930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- helpers ----------\n",
    "def _first_present(cols: List[str], *candidates_groups: List[str]) -> Optional[str]:\n",
    "    cols_l = [c.lower().strip() for c in cols]\n",
    "    for candidates in candidates_groups:\n",
    "        for cand in candidates:\n",
    "            if cand in cols_l:\n",
    "                return cols[cols_l.index(cand)]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aea97a",
   "metadata": {},
   "source": [
    "**_first_present** – Identifies the first available column in the provided candidate sets so later steps can rely on consistent schema names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_datetime(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\", utc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3deb503b",
   "metadata": {},
   "source": [
    "**_coerce_datetime** – Parses and coerces timestamp-like fields into pandas datetimes while dropping invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b587a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(s: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    lo, hi = s.quantile([lower_q, upper_q])\n",
    "    return s.clip(lower=lo, upper=hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c5e22f",
   "metadata": {},
   "source": [
    "**winsorize** – Clips the tails of a numeric series to dampen outliers before volatility and ratio calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6659418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- indicators ----------\n",
    "def compute_rsi(prices: pd.Series, window: int = 14) -> pd.Series:\n",
    "    delta = prices.diff()\n",
    "    gain = delta.clip(lower=0).ewm(alpha=1/window, adjust=False, min_periods=window).mean()\n",
    "    loss = -delta.clip(upper=0).ewm(alpha=1/window, adjust=False, min_periods=window).mean()\n",
    "    rs = gain / (loss.replace(0, np.nan))\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a51a71",
   "metadata": {},
   "source": [
    "**compute_rsi** – Calculates a smoothed relative strength index to capture short-term momentum in the adjusted close series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e59c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macd(prices: pd.Series, span_fast: int = 12, span_slow: int = 26, span_signal: int = 9) -> pd.DataFrame:\n",
    "    ema_fast = prices.ewm(span=span_fast, adjust=False).mean()\n",
    "    ema_slow = prices.ewm(span=span_slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal = macd_line.ewm(span=span_signal, adjust=False).mean()\n",
    "    hist = macd_line - signal\n",
    "    return pd.DataFrame({\"macd\": macd_line, \"macd_signal\": signal, \"macd_hist\": hist})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfb53c6",
   "metadata": {},
   "source": [
    "**compute_macd** – Generates MACD, signal, and histogram values from the price series to summarise trend and momentum shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e46f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bbands(prices: pd.Series, window: int = 20, n_std: float = 2.0) -> pd.DataFrame:\n",
    "    ma = prices.rolling(window, min_periods=window).mean()\n",
    "    sd = prices.rolling(window, min_periods=window).std()\n",
    "    upper = ma + n_std * sd\n",
    "    lower = ma - n_std * sd\n",
    "    return pd.DataFrame({\"bb_mid\": ma, \"bb_upper\": upper, \"bb_lower\": lower})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332ad959",
   "metadata": {},
   "source": [
    "**compute_bbands** – Computes Bollinger Bands around a rolling mean to contextualise price levels against recent volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89124d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- adjustment ----------\n",
    "def adjust_ohlc_with_adjclose(df: pd.DataFrame,\n",
    "                              col_open: Optional[str],\n",
    "                              col_high: Optional[str],\n",
    "                              col_low: Optional[str],\n",
    "                              col_close: Optional[str],\n",
    "                              col_adjclose: Optional[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if col_adjclose is None or col_close is None:\n",
    "        return out\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        factor = out[col_adjclose] / out[col_close]\n",
    "    factor = factor.replace([np.inf, -np.inf], np.nan)\n",
    "    factor = factor.where(factor > 0, np.nan).ffill().bfill()\n",
    "    if col_open is not None:\n",
    "        out[\"adj_open\"] = out[col_open] * factor\n",
    "    if col_high is not None:\n",
    "        out[\"adj_high\"] = out[col_high] * factor\n",
    "    if col_low is not None:\n",
    "        out[\"adj_low\"] = out[col_low] * factor\n",
    "    out[\"adj_close\"] = out[col_close] * factor\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d3f5d4",
   "metadata": {},
   "source": [
    "**adjust_ohlc_with_adjclose** – Back-adjusts open, high, low, and close prices using the adjusted-close factor so the OHLC stack remains split-adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce4676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- core ----------\n",
    "def preprocess_cat_excel(input_path: str, output_base: str):\n",
    "    # 1) load\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "\n",
    "    original_cols = list(df.columns)\n",
    "\n",
    "    # 2) detect columns\n",
    "    date_col = _first_present(original_cols, [\"date\", \"timestamp\", \"time\", \"datetime\"])\n",
    "    open_col = _first_present(original_cols, [\"open\", \"px_open\", \"o\"])\n",
    "    high_col = _first_present(original_cols, [\"high\", \"px_high\", \"h\"])\n",
    "    low_col  = _first_present(original_cols, [\"low\", \"px_low\", \"l\"])\n",
    "    close_col = _first_present(original_cols, [\"adj close\", \"adjusted close\", \"close\", \"px_last\", \"price\", \"last\"])\n",
    "    # if adj close exists, prefer the raw close for factor calc\n",
    "    if _first_present(original_cols, [\"adj close\", \"adjusted close\"]):\n",
    "        close_col = _first_present(original_cols, [\"close\", \"px_last\", \"price\", \"last\"]) or close_col\n",
    "    adj_close_col = _first_present(original_cols, [\"adj close\", \"adjusted close\"])\n",
    "    volume_col = _first_present(original_cols, [\"volume\", \"vol\", \"qty\", \"turnover\"])\n",
    "\n",
    "    # 3) dates\n",
    "    if date_col is None:\n",
    "        maybe_date = original_cols[0]\n",
    "        parsed = pd.to_datetime(df[maybe_date], errors=\"coerce\")\n",
    "        if parsed.notna().mean() > 0.7:\n",
    "            date_col = maybe_date\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"No date/time column detected.\")\n",
    "\n",
    "    df[date_col] = _coerce_datetime(df[date_col])\n",
    "    df = (df.dropna(subset=[date_col])\n",
    "            .sort_values(date_col)\n",
    "            .drop_duplicates(subset=[date_col])\n",
    "            .reset_index(drop=True))\n",
    "\n",
    "    # 4) numerics & patching\n",
    "    price_cols = [c for c in [open_col, high_col, low_col, close_col, adj_close_col] if c is not None]\n",
    "    for c in price_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if volume_col is not None:\n",
    "        df[volume_col] = pd.to_numeric(df[volume_col], errors=\"coerce\")\n",
    "\n",
    "    df[price_cols] = df[price_cols].ffill().bfill()\n",
    "    if volume_col is not None:\n",
    "        df.loc[df[volume_col] == 0, volume_col] = np.nan\n",
    "        df[volume_col] = df[volume_col].ffill().bfill()\n",
    "\n",
    "    # 5) adjusted OHLC\n",
    "    df = adjust_ohlc_with_adjclose(df, open_col, high_col, low_col, close_col, adj_close_col)\n",
    "    if \"adj_close\" not in df.columns:\n",
    "        if close_col is None:\n",
    "            raise ValueError(\"No usable close or adjusted close column.\")\n",
    "        df[\"adj_close\"] = df[close_col]\n",
    "\n",
    "    # 6) features\n",
    "    df[\"ret\"] = np.log(df[\"adj_close\"]).diff()\n",
    "    df[\"ret_w\"] = winsorize(df[\"ret\"])\n",
    "    df[\"vol_20\"] = df[\"ret\"].rolling(20, min_periods=20).std()\n",
    "    df[\"vol_60\"] = df[\"ret\"].rolling(60, min_periods=60).std()\n",
    "    df[\"rv_20_annual\"] = df[\"vol_20\"] * np.sqrt(252.0)\n",
    "    df[\"rv_60_annual\"] = df[\"vol_60\"] * np.sqrt(252.0)\n",
    "    df[\"ret_vol_scaled\"] = df[\"ret\"] / (df[\"vol_20\"].replace(0, np.nan))\n",
    "    df[\"ret_vol_scaled_w\"] = winsorize(df[\"ret_vol_scaled\"])\n",
    "\n",
    "    rsi14 = compute_rsi(df[\"adj_close\"], window=14)\n",
    "    macd_df = compute_macd(df[\"adj_close\"], span_fast=12, span_slow=26, span_signal=9)\n",
    "    bb_df = compute_bbands(df[\"adj_close\"], window=20, n_std=2.0)\n",
    "    df = pd.concat([df, rsi14.rename(\"rsi14\"), macd_df, bb_df], axis=1)\n",
    "\n",
    "    # 7) tidy output\n",
    "    adj_cols_available = [c for c in [\"adj_open\", \"adj_high\", \"adj_low\"] if c in df.columns]\n",
    "    out_cols = [date_col] + adj_cols_available + [\"adj_close\"]\n",
    "    if volume_col is not None:\n",
    "        out_cols.append(volume_col)\n",
    "    out_cols += [\"ret\", \"ret_w\", \"ret_vol_scaled\", \"ret_vol_scaled_w\",\n",
    "                 \"vol_20\", \"vol_60\", \"rv_20_annual\", \"rv_60_annual\",\n",
    "                 \"rsi14\", \"macd\", \"macd_signal\", \"macd_hist\",\n",
    "                 \"bb_mid\", \"bb_upper\", \"bb_lower\"]\n",
    "\n",
    "    tidy = df[out_cols].copy()\n",
    "    rename_map = {date_col: \"date\"}\n",
    "    if volume_col is not None:\n",
    "        rename_map[volume_col] = \"volume\"\n",
    "    tidy = tidy.rename(columns=rename_map)\n",
    "    tidy[\"date\"] = pd.to_datetime(tidy[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    # 8) write files\n",
    "    csv_path = f\"{output_base}.csv\"\n",
    "    tidy.to_csv(csv_path, index=False)\n",
    "\n",
    "    parquet_path = None\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        parquet_path = f\"{output_base}.parquet\"\n",
    "        tidy.to_parquet(parquet_path, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return csv_path, parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40962fb6",
   "metadata": {},
   "source": [
    "**preprocess_cat_excel** – Orchestrates the end-to-end pipeline: load the Excel file, detect key columns, clean data, engineer features, and persist outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b59a0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    ap = argparse.ArgumentParser(description=\"Preprocess equity time series (CAT US.xlsx style).\")\n",
    "    ap.add_argument(\"--input\", required=True, help=\"Path to Excel file\")\n",
    "    ap.add_argument(\"--output_base\", required=True, help=\"Output path prefix (no extension)\")\n",
    "    args = ap.parse_args()\n",
    "    csv_path, pq_path = preprocess_cat_excel(args.input, args.output_base)\n",
    "    print(\"Wrote:\", csv_path)\n",
    "    if pq_path:\n",
    "        print(\"Wrote:\", pq_path)\n",
    "    else:\n",
    "        print(\"Parquet not written (pyarrow missing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca21e1",
   "metadata": {},
   "source": [
    "**main** – Provides the command-line entry point that wires argument parsing to the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcd9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep backwards-compatible name\n",
    "def preprocess_cat_excel(input_path: str, output_base: str):\n",
    "    return preprocess_cat_excel.__impl__(input_path, output_base)  # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72849996",
   "metadata": {},
   "source": [
    "**preprocess_cat_excel** – Maintains a backward-compatible public alias by delegating to an implementation pointer set at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372975f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach implementation\n",
    "def _attach_impl():\n",
    "    def impl(input_path, output_base):\n",
    "        return __real_preprocess(input_path, output_base)\n",
    "    preprocess_cat_excel.__impl__ = impl  # type: ignore[attr-defined]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1fdb2",
   "metadata": {},
   "source": [
    "**_attach_impl** – Attaches the concrete processing implementation to the public alias so notebooks or libraries can override behaviour if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ef3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __real_preprocess(input_path: str, output_base: str):\n",
    "    # Call the main processing directly\n",
    "    return _process(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5298850d",
   "metadata": {},
   "source": [
    "**__real_preprocess** – Serves as the first shim in the delegation chain, routing calls into the internal `_process` wrapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49eab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _process(input_path: str, output_base: str):\n",
    "    # Simple inline call to keep things tidy\n",
    "    # We can just call the function defined above without indirection,\n",
    "    # but we keep this structure to allow overriding in notebooks if needed.\n",
    "    return _do_process(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca66f3f",
   "metadata": {},
   "source": [
    "**_process** – Adds an intermediary hook meant for potential extension before handing work to `_do_process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb424a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _do_process(input_path: str, output_base: str):\n",
    "    # Final call\n",
    "    # Reuse logic by calling the top-level 'preprocess_cat_excel' implementation\n",
    "    # which we aliased via __impl__ to avoid recursion.\n",
    "    return _simple_impl(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078622a",
   "metadata": {},
   "source": [
    "**_do_process** – Prevents recursive aliasing by forwarding the call to `_simple_impl` in a controlled manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54b48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _simple_impl(input_path: str, output_base: str):\n",
    "    # The real worker is the top 'preprocess_cat_excel' code path via __impl__\n",
    "    # Here, directly replicate the processing for clarity.\n",
    "    # However, to prevent recursion, we directly execute the computation here again.\n",
    "    try:\n",
    "        # We will just call the earlier function's internals to keep it straightforward.\n",
    "        return __compute(input_path, output_base)\n",
    "    except RecursionError:\n",
    "        # Fallback to recomputing without aliasing\n",
    "        return preprocess_cat_excel_simple(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafcafc",
   "metadata": {},
   "source": [
    "**_simple_impl** – Attempts to execute the core computation, retrying with the fallback implementation if recursion is detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c4a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def __compute(input_path: str, output_base: str):\n",
    "    # Re-run the main processing function once more (safe call path)\n",
    "    return actually_process(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a826ae",
   "metadata": {},
   "source": [
    "**__compute** – Runs the canonical processing path through `actually_process`, keeping the delegation chain explicit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103db68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def actually_process(input_path: str, output_base: str):\n",
    "    # Directly call the core function defined at the top\n",
    "    return preprocess_cat_excel_core(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ed6d4d",
   "metadata": {},
   "source": [
    "**actually_process** – Delegates to `preprocess_cat_excel_core` so the compatibility alias still resolves to the core logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9e1c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cat_excel_core(input_path: str, output_base: str):\n",
    "    # To keep things robust, duplicate the minimal logic inline:\n",
    "    return preprocess_cat_excel_simple(input_path, output_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e223fbd",
   "metadata": {},
   "source": [
    "**preprocess_cat_excel_core** – Thin wrapper that re-exports the simplified preprocessing function for clarity within the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720d08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cat_excel_simple(input_path: str, output_base: str):\n",
    "    # Duplicate minimal logic by reading and writing through the main function\n",
    "    # (avoids recursive calls in some environments)\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "\n",
    "    original_cols = list(df.columns)\n",
    "\n",
    "    date_col = _first_present(original_cols, [\"date\", \"timestamp\", \"time\", \"datetime\"])\n",
    "    open_col = _first_present(original_cols, [\"open\", \"px_open\", \"o\"])\n",
    "    high_col = _first_present(original_cols, [\"high\", \"px_high\", \"h\"])\n",
    "    low_col  = _first_present(original_cols, [\"low\", \"px_low\", \"l\"])\n",
    "    close_col = _first_present(original_cols, [\"adj close\", \"adjusted close\", \"close\", \"px_last\", \"price\", \"last\"])\n",
    "    if _first_present(original_cols, [\"adj close\", \"adjusted close\"]):\n",
    "        close_col = _first_present(original_cols, [\"close\", \"px_last\", \"price\", \"last\"]) or close_col\n",
    "    adj_close_col = _first_present(original_cols, [\"adj close\", \"adjusted close\"])\n",
    "    volume_col = _first_present(original_cols, [\"volume\", \"vol\", \"qty\", \"turnover\"])\n",
    "\n",
    "    if date_col is None:\n",
    "        maybe_date = original_cols[0]\n",
    "        parsed = pd.to_datetime(df[maybe_date], errors=\"coerce\")\n",
    "        if parsed.notna().mean() > 0.7:\n",
    "            date_col = maybe_date\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"No date/time column detected.\")\n",
    "\n",
    "    df[date_col] = _coerce_datetime(df[date_col])\n",
    "    df = (df.dropna(subset=[date_col]).sort_values(date_col).drop_duplicates(subset=[date_col]).reset_index(drop=True))\n",
    "\n",
    "    price_cols = [c for c in [open_col, high_col, low_col, close_col, adj_close_col] if c is not None]\n",
    "    for c in price_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if volume_col is not None:\n",
    "        df[volume_col] = pd.to_numeric(df[volume_col], errors=\"coerce\")\n",
    "\n",
    "    df[price_cols] = df[price_cols].ffill().bfill()\n",
    "    if volume_col is not None:\n",
    "        df.loc[df[volume_col] == 0, volume_col] = np.nan\n",
    "        df[volume_col] = df[volume_col].ffill().bfill()\n",
    "\n",
    "    df = adjust_ohlc_with_adjclose(df, open_col, high_col, low_col, close_col, adj_close_col)\n",
    "    if \"adj_close\" not in df.columns:\n",
    "        if close_col is None:\n",
    "            raise ValueError(\"No usable close or adjusted close column.\")\n",
    "        df[\"adj_close\"] = df[close_col]\n",
    "\n",
    "    df[\"ret\"] = np.log(df[\"adj_close\"]).diff()\n",
    "    df[\"ret_w\"] = winsorize(df[\"ret\"])\n",
    "    df[\"vol_20\"] = df[\"ret\"].rolling(20, min_periods=20).std()\n",
    "    df[\"vol_60\"] = df[\"ret\"].rolling(60, min_periods=60).std()\n",
    "    df[\"rv_20_annual\"] = df[\"vol_20\"] * np.sqrt(252.0)\n",
    "    df[\"rv_60_annual\"] = df[\"vol_60\"] * np.sqrt(252.0)\n",
    "    df[\"ret_vol_scaled\"] = df[\"ret\"] / (df[\"vol_20\"].replace(0, np.nan))\n",
    "    df[\"ret_vol_scaled_w\"] = winsorize(df[\"ret_vol_scaled\"])\n",
    "\n",
    "    rsi14 = compute_rsi(df[\"adj_close\"], window=14)\n",
    "    macd_df = compute_macd(df[\"adj_close\"], span_fast=12, span_slow=26, span_signal=9)\n",
    "    bb_df = compute_bbands(df[\"adj_close\"], window=20, n_std=2.0)\n",
    "    df = pd.concat([df, rsi14.rename(\"rsi14\"), macd_df, bb_df], axis=1)\n",
    "\n",
    "    adj_cols_available = [c for c in [\"adj_open\", \"adj_high\", \"adj_low\"] if c in df.columns]\n",
    "    out_cols = [date_col] + adj_cols_available + [\"adj_close\"]\n",
    "    if volume_col is not None:\n",
    "        out_cols.append(volume_col)\n",
    "    out_cols += [\"ret\", \"ret_w\", \"ret_vol_scaled\", \"ret_vol_scaled_w\",\n",
    "                 \"vol_20\", \"vol_60\", \"rv_20_annual\", \"rv_60_annual\",\n",
    "                 \"rsi14\", \"macd\", \"macd_signal\", \"macd_hist\",\n",
    "                 \"bb_mid\", \"bb_upper\", \"bb_lower\"]\n",
    "\n",
    "    tidy = df[out_cols].copy()\n",
    "    rename_map = {date_col: \"date\"}\n",
    "    if volume_col is not None:\n",
    "        rename_map[volume_col] = \"volume\"\n",
    "    tidy = tidy.rename(columns=rename_map)\n",
    "    tidy[\"date\"] = pd.to_datetime(tidy[\"date\"]).dt.tz_localize(None)\n",
    "\n",
    "    csv_path = f\"{output_base}.csv\"\n",
    "    tidy.to_csv(csv_path, index=False)\n",
    "\n",
    "    parquet_path = None\n",
    "    try:\n",
    "        import pyarrow  # noqa: F401\n",
    "        parquet_path = f\"{output_base}.parquet\"\n",
    "        tidy.to_parquet(parquet_path, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return csv_path, parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d5a7f",
   "metadata": {},
   "source": [
    "**preprocess_cat_excel_simple** – Standalone copy of the preprocessing routine used by the compatibility chain to avoid recursion while matching outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e16c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wire the implementation for the public function alias\n",
    "_attach_impl()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
