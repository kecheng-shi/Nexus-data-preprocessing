{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57317a0",
   "metadata": {},
   "source": [
    "# Nexus Data Analysis\n",
    "_Kecheng Shi_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369ad67",
   "metadata": {},
   "source": [
    "# 1. Data Inventory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137353ed",
   "metadata": {},
   "source": [
    "## 1.1 Category Snapshot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf98042",
   "metadata": {},
   "source": [
    "- **Equities** (52) — Vanguard Total Stock Market In - VTSAX US Equity US, Home Depot Inc_The - HD US Equity Equity, JPMorgan Chase & Co - JPM US Equity Equity, …\n",
    "- **Market Indices** (23) — Morningstar LSTA US Leveraged - SPBDLL Index Index, US Financial Conditions FCON - BFCIUS INDEX Index, Bloomberg US Govt Inflation-Li - BCIT1T Index Index, …\n",
    "- **Futures & Forwards** (16) — CSI 300 IDX FUTUR Dec25 - IFBZ5 Index Index, Generic 1st 'W ' Future - W 1 Comdty Comdty, Generic 1st 'GX' Future - GX1 Index Index, …\n",
    "- **Macro Indicators** (15) — Manufacturing SA - CPMINDX INDEX Index, YoY % NSA - CPI YOY Index Index, Month % change - DGNOCHNG Index Index, …\n",
    "- **Funds & ETFs** (13) — Vanguard Balanced Index Fund - VBIAX US Equity US, NYLI Hedge Multi-Strategy Trac - QAI US EQUITY Index, SPDR Gold Shares - GLD US Equity Equity, …\n",
    "- **FX & Rates** (12) — AUSTRALIAN DOLLAR 1 MO - AUD1M BGN Curncy BGN Curncy, Bloomberg Nominal USD 5Y Spot - BTSIUS05 Index Curncy, EURO 3 MO - EUR3M BGN Curncy BGN Curncy, …\n",
    "- **Convertible Credit** (10) — ETSY 0 1_8 10_01_26 - ETSY 0.125 10_01_26 Corp Corp (Conv), UBER 0 7_8 12_01_28 - UBER 0.875 12_01_28 Corp Corp (Conv), AMD 3.924 06_01_32 - AMD 3.924 06_01_32 Corp Corp (Conv), …\n",
    "- **Commodities** (9) — US DOLLAR_China Offshore Spot - USDCNH Comdty Comdty, US 10YR FUT OPTN  Dec25C   112 - TYZ5C 112 Comdty Comdty, 3 Month SOFR Opt  Dec25P    95 - SFRZ5P 95 Comdty Comdty, …\n",
    "- **Options & Derivatives** (9) — International Business Machine - IBM US 12_19_25 C150 Equity Equity, S&P 500 INDEX - SPX 12_19_25 P6000 Index Index, Shanghai Stock Exchange SSE 50 - SSE50 9 C3000 INDEX Index, …\n",
    "- **Corporate Credit** (8) — CAT 3 1_4 09_19_49 - CAT 3.25 09_19_49 Corp Corp, T 4 1_2 05_15_35 - T 4.50 05_15_35 Corp Corp, CCO 7 1_2 06_01_29 - CCO 7.50 06_01_29 144A Corp Corp, …\n",
    "- **Volatility Indices** (5) — Cboe Volatility Index - VIX 12_17_25 P15 Index Index, ICE BofA MOVE Index - MOVE Index Index, Cboe Volatility Index - VIX Index Index, …\n",
    "- **Government Bonds** (4) — SOAF 7.3 04_20_52 - 836205BE3 Govt Govt, BRAZIL 6 04_07_26 - BRAZIL 6 04_07_26 Govt Govt, MEX 6.05 01_11_40 - 91086QAV0 Govt Govt, …\n",
    "- **Digital Assets** (3) — Bloomberg Galaxy Crypto Index - BGCI Index Index, Bitcoin_US DOLLAR - XBTUSD BGN Curncy BGN Curncy, Ethereum_US DOLLAR - XETUSD BGN Curncy BGN Curncy\n",
    "- **Municipal Bonds** (2) — #N_A Field Not Applicable - 650028ZN6 Muni Muni, #N_A Field Not Applicable - 13063D4D Muni Muni\n",
    "- **Credit Derivatives** (1) — #N_A N_A - IBM CDS USD SR 5Y D14 Corp CDS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79963885",
   "metadata": {},
   "source": [
    "## 1.2 Global Equities & Funds (65 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c9fa8e",
   "metadata": {},
   "source": [
    "- Vanguard Total Stock Market In - VTSAX US Equity US — broad U.S. equity mutual fund.\n",
    "- SPDR S&P 500 ETF Trust - SPY US Equity Equity — flagship S&P 500 ETF benchmark.\n",
    "- Microsoft Corp - MSFT US Equity Equity — U.S. mega-cap technology stock.\n",
    "- Alibaba Group Holding Ltd - BABA US Equity Equity — China e-commerce ADR listed in the U.S.\n",
    "- iShares MSCI Emerging Markets - EEM US Equity Equity — diversified emerging markets equity ETF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efa05e3",
   "metadata": {},
   "source": [
    "## 1.3 Market Indices & Macro Indicators (38 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d420f8",
   "metadata": {},
   "source": [
    "- US Financial Conditions FCON - BFCIUS INDEX Index — Bloomberg U.S. financial conditions composite.\n",
    "- Morningstar LSTA US Leveraged - SPBDLL Index Index — U.S. leveraged loan benchmark.\n",
    "- Bloomberg US Govt Inflation-Li - BCIT1T Index Index — 1–10 year U.S. TIPS index.\n",
    "- Manufacturing SA - CPMINDX INDEX Index — manufacturing PMI diffusion index (seasonally adjusted).\n",
    "- Citi Economic Surprise - CESIUSD INDEX Index — macro surprise tracker for the United States.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9e3e8",
   "metadata": {},
   "source": [
    "## 1.4 Rates & FX Benchmarks (12 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d0e1d",
   "metadata": {},
   "source": [
    "- AUSTRALIAN DOLLAR 1 MO - AUD1M BGN Curncy BGN Curncy — short-term AUD money-market rate.\n",
    "- EURO_US DOLLAR - EURUSD Curncy Curncy — EUR/USD spot exchange rate.\n",
    "- Bloomberg Nominal USD 5Y Spot - BTSIUS05 Index Curncy — U.S. Treasury 5-year nominal yield.\n",
    "- Fed Funds Target Rate US - FDTR Index Index — Federal Funds upper bound target.\n",
    "- SHIBOR Fixing 3M - SHIF3M INDEX Index — 3-month Shanghai interbank offered rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec040f3",
   "metadata": {},
   "source": [
    "## 1.5 Commodities & Futures (25 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40067ea4",
   "metadata": {},
   "source": [
    "- Generic 1st 'CL' Future - CL1 Comdty Comdty — front-month WTI crude oil future.\n",
    "- Generic 1st 'NG' Future - NG1 Comdty Comdty — front-month Henry Hub natural gas future.\n",
    "- Generic 1st 'GC' Future - GC1 Comdty Comdty — front-month COMEX gold future.\n",
    "- Generic 1st 'W ' Future - W 1 Comdty Comdty — front-month CBOT wheat future.\n",
    "- CSI 300 IDX FUTUR Dec25 - IFBZ5 Index Index — December 2025 CSI 300 equity index future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b263877",
   "metadata": {},
   "source": [
    "## 1.6 Options & Volatility (14 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e945a",
   "metadata": {},
   "source": [
    "- S&P 500 INDEX - SPX 12_19_25 P6000 Index Index — SPX put option expiring December 2025.\n",
    "- Deutsche Boerse AG German Stoc - DAX 10_17_25 P23000 INDEX Index — DAX put option expiring October 2025.\n",
    "- Nikkei 225 - NKY 10 P37250 INDEX Index — Nikkei 225 put option (October tenor).\n",
    "- Cboe Volatility Index - VIX Index Index — 30-day implied volatility for the S&P 500.\n",
    "- ICE BofA MOVE Index - MOVE Index Index — U.S. Treasury rate volatility benchmark.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafed632",
   "metadata": {},
   "source": [
    "## 1.7 Credit Complex (25 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2dcce",
   "metadata": {},
   "source": [
    "- CAT 3 1_4 09_19_49 - CAT 3.25 09_19_49 Corp Corp — Caterpillar long-dated corporate bond.\n",
    "- ETSY 0 1_8 10_01_26 - ETSY 0.125 10_01_26 Corp Corp (Conv) — Etsy convertible note.\n",
    "- BRAZIL 6 04_07_26 - BRAZIL 6 04_07_26 Govt Govt — Brazil U.S.-dollar sovereign bond.\n",
    "- #N_A Field Not Applicable - 650028ZN6 Muni Muni — U.S. municipal bond from the Nexus pack.\n",
    "- #N_A N_A - IBM CDS USD SR 5Y D14 Corp CDS — IBM senior 5-year CDS spread.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9adaaca",
   "metadata": {},
   "source": [
    "## 1.8 Digital Assets & Alternatives (3 series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe19e5d",
   "metadata": {},
   "source": [
    "- Bloomberg Galaxy Crypto Index - BGCI Index Index — broad digital asset market index.\n",
    "- Bitcoin_US DOLLAR - XBTUSD BGN Curncy BGN Curncy — Bitcoin spot versus U.S. dollar.\n",
    "- Ethereum_US DOLLAR - XETUSD BGN Curncy BGN Curncy — Ethereum spot versus U.S. dollar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c15606",
   "metadata": {},
   "source": [
    "# 2. Data Preprocessing Process\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Pipeline Overview\n",
    "\n",
    "- Stage 1–3: detect canonical columns, coerce datetimes, and enforce ordered, duplicate-free timelines.\n",
    "- Stage 4–6: smooth noise via winsorised returns, engineer volatility/momentum features, and scale outliers.\n",
    "- Stage 7–10: persist cleaned frames with metadata so market-dynamics blocks can pull aligned panels quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data Sources & Paths\n",
    "\n",
    "- Raw inputs live in `FULL Nexus Data` (`*.xlsx`, one instrument per file).\n",
    "- Normalised outputs are written to `analysis/preprocessed` as CSV (and Parquet when `pyarrow` is available).\n",
    "- Metadata tables capture coverage, cadence, and feature availability so downstream notebooks can sanity-check inputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b43f4dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 182 raw Excel files.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (10, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>series</th><th>raw_file</th></tr><tr><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;#N_A Field Not Applicable - 13…</td><td>&quot;#N_A Field Not Applicable - 13…</td></tr><tr><td>&quot;#N_A Field Not Applicable - 65…</td><td>&quot;#N_A Field Not Applicable - 65…</td></tr><tr><td>&quot;#N_A N_A - IBM CDS USD SR 5Y D…</td><td>&quot;#N_A N_A - IBM CDS USD SR 5Y D…</td></tr><tr><td>&quot;3 Month SOFR Opt  Dec25C    95…</td><td>&quot;3 Month SOFR Opt  Dec25C    95…</td></tr><tr><td>&quot;3 Month SOFR Opt  Dec25P    95…</td><td>&quot;3 Month SOFR Opt  Dec25P    95…</td></tr><tr><td>&quot;AEP 1 11_01_25 - AEP 1.00 11_0…</td><td>&quot;AEP 1 11_01_25 - AEP 1.00 11_0…</td></tr><tr><td>&quot;AMD 3.924 06_01_32 - AMD 3.924…</td><td>&quot;AMD 3.924 06_01_32 - AMD 3.924…</td></tr><tr><td>&quot;AT&amp;T Inc - T US Equity Equity&quot;</td><td>&quot;AT&amp;T Inc - T US Equity Equity.…</td></tr><tr><td>&quot;AUSTRALIAN DOLLAR 1 MO - AUD1M…</td><td>&quot;AUSTRALIAN DOLLAR 1 MO - AUD1M…</td></tr><tr><td>&quot;AUSTRALIAN DOLLAR 3 MO - AUD3M…</td><td>&quot;AUSTRALIAN DOLLAR 3 MO - AUD3M…</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (10, 2)\n",
       "┌─────────────────────────────────┬─────────────────────────────────┐\n",
       "│ series                          ┆ raw_file                        │\n",
       "│ ---                             ┆ ---                             │\n",
       "│ str                             ┆ str                             │\n",
       "╞═════════════════════════════════╪═════════════════════════════════╡\n",
       "│ #N_A Field Not Applicable - 13… ┆ #N_A Field Not Applicable - 13… │\n",
       "│ #N_A Field Not Applicable - 65… ┆ #N_A Field Not Applicable - 65… │\n",
       "│ #N_A N_A - IBM CDS USD SR 5Y D… ┆ #N_A N_A - IBM CDS USD SR 5Y D… │\n",
       "│ 3 Month SOFR Opt  Dec25C    95… ┆ 3 Month SOFR Opt  Dec25C    95… │\n",
       "│ 3 Month SOFR Opt  Dec25P    95… ┆ 3 Month SOFR Opt  Dec25P    95… │\n",
       "│ AEP 1 11_01_25 - AEP 1.00 11_0… ┆ AEP 1 11_01_25 - AEP 1.00 11_0… │\n",
       "│ AMD 3.924 06_01_32 - AMD 3.924… ┆ AMD 3.924 06_01_32 - AMD 3.924… │\n",
       "│ AT&T Inc - T US Equity Equity   ┆ AT&T Inc - T US Equity Equity.… │\n",
       "│ AUSTRALIAN DOLLAR 1 MO - AUD1M… ┆ AUSTRALIAN DOLLAR 1 MO - AUD1M… │\n",
       "│ AUSTRALIAN DOLLAR 3 MO - AUD3M… ┆ AUSTRALIAN DOLLAR 3 MO - AUD3M… │\n",
       "└─────────────────────────────────┴─────────────────────────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, Iterable, List, Optional, Tuple\n",
    "\n",
    "import math\n",
    "import statistics\n",
    "from datetime import date, datetime\n",
    "\n",
    "import polars as pl\n",
    "from IPython.display import display\n",
    "from dateutil import parser\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.datetime import from_excel\n",
    "\n",
    "RAW_DATA_DIR = Path(\"FULL Nexus Data\")\n",
    "OUTPUT_DIR = Path(\"analysis/preprocessed\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "excel_files = sorted(RAW_DATA_DIR.glob(\"*.xlsx\"))\n",
    "catalog = pl.DataFrame({\n",
    "    \"series\": [f.stem for f in excel_files],\n",
    "    \"raw_file\": [f.name for f in excel_files],\n",
    "})\n",
    "print(f\"Detected {catalog.height} raw Excel files.\")\n",
    "if catalog.height == 0:\n",
    "    raise FileNotFoundError(\"No raw Excel files found in 'FULL Nexus Data'.\")\n",
    "display(catalog.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1512bd1a",
   "metadata": {},
   "source": [
    "## 2.3 Helper Functions\n",
    "\n",
    "Canonical schema detection, datetime coercion, and feature engineering mirrors the dedicated preprocessing notebook with lightweight tweaks for batch execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a563a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCHEMA_ALIASES: Dict[str, Tuple[Tuple[str, ...], ...]] = {\n",
    "    \"date\": ((\"date\", \"timestamp\", \"time\", \"datetime\"),),\n",
    "    \"open\": ((\"open\", \"px_open\", \"o\"),),\n",
    "    \"high\": ((\"high\", \"px_high\", \"h\"),),\n",
    "    \"low\": ((\"low\", \"px_low\", \"l\"),),\n",
    "    \"close\": ((\"close\", \"px_last\", \"last\", \"price\"),),\n",
    "    \"adj_close\": ((\"adj close\", \"adjusted close\"),),\n",
    "    \"volume\": ((\"volume\", \"vol\", \"qty\", \"turnover\"),),\n",
    "}\n",
    "\n",
    "def _first_present(columns: Iterable[str], *candidates_groups: Iterable[str]) -> Optional[str]:\n",
    "    cols = list(columns)\n",
    "    lowered = [c.lower().strip() for c in cols]\n",
    "    for group in candidates_groups:\n",
    "        for candidate in group:\n",
    "            key = candidate.lower().strip()\n",
    "            if key in lowered:\n",
    "                return cols[lowered.index(key)]\n",
    "    return None\n",
    "\n",
    "def _coerce_datetime(values: List[object]) -> List[Optional[datetime]]:\n",
    "    out: List[Optional[datetime]] = []\n",
    "    for value in values:\n",
    "        if value is None:\n",
    "            out.append(None)\n",
    "            continue\n",
    "        if isinstance(value, datetime):\n",
    "            out.append(value.replace(tzinfo=None))\n",
    "            continue\n",
    "        if isinstance(value, date):\n",
    "            out.append(datetime.combine(value, datetime.min.time()))\n",
    "            continue\n",
    "        if isinstance(value, (int, float)):\n",
    "            try:\n",
    "                out.append(from_excel(value))\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "        try:\n",
    "            parsed_dt = parser.parse(str(value))\n",
    "            out.append(parsed_dt.replace(tzinfo=None))\n",
    "        except Exception:\n",
    "            out.append(None)\n",
    "    return out\n",
    "\n",
    "def winsorize_series(series: pl.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pl.Series:\n",
    "    clean = series.drop_nulls()\n",
    "    if clean.is_empty():\n",
    "        return series\n",
    "    lower = clean.quantile(lower_q)\n",
    "    upper = clean.quantile(upper_q)\n",
    "    return series.clip(lower, upper)\n",
    "\n",
    "def load_excel_to_polars(path: Path) -> pl.DataFrame:\n",
    "    wb = load_workbook(path, data_only=True, read_only=True)\n",
    "    sheet = wb.active\n",
    "    rows = list(sheet.iter_rows(values_only=True))\n",
    "    wb.close()\n",
    "    if not rows:\n",
    "        return pl.DataFrame()\n",
    "    header = [str(cell).strip() if cell is not None else \"\" for cell in rows[0]]\n",
    "    data_rows = [list(row) for row in rows[1:] if any(cell is not None for cell in row)]\n",
    "    if not data_rows:\n",
    "        return pl.DataFrame({name: [] for name in header})\n",
    "    columns = list(zip(*data_rows))\n",
    "    data = {header[i]: list(columns[i]) for i in range(len(header))}\n",
    "    return pl.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6d9736",
   "metadata": {},
   "source": [
    "## 2.4 Single-Series Preprocessing\n",
    "\n",
    "Process an individual instrument: detect columns, align timestamps, engineer features, and emit a tidy schema with metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c1d2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_single_series(path: Path) -> Tuple[pl.DataFrame, Dict[str, object]]:\n",
    "    \"\"\"Load one Excel file and return (tidy_frame, metadata).\"\"\"\n",
    "    raw = load_excel_to_polars(path)\n",
    "    if raw.is_empty():\n",
    "        raise ValueError(\"Workbook is empty or unreadable.\")\n",
    "\n",
    "    original_cols = raw.columns\n",
    "    date_col = _first_present(original_cols, *SCHEMA_ALIASES[\"date\"])\n",
    "    open_col = _first_present(original_cols, *SCHEMA_ALIASES[\"open\"])\n",
    "    high_col = _first_present(original_cols, *SCHEMA_ALIASES[\"high\"])\n",
    "    low_col = _first_present(original_cols, *SCHEMA_ALIASES[\"low\"])\n",
    "    adj_close_col = _first_present(original_cols, *SCHEMA_ALIASES[\"adj_close\"])\n",
    "    close_col = _first_present(original_cols, *SCHEMA_ALIASES[\"close\"])\n",
    "    if adj_close_col and close_col is None:\n",
    "        close_col = adj_close_col\n",
    "    volume_col = _first_present(original_cols, *SCHEMA_ALIASES[\"volume\"])\n",
    "    if close_col is None and adj_close_col is None:\n",
    "        candidate_cols = [c for c in original_cols if c != date_col]\n",
    "        if candidate_cols:\n",
    "            close_col = candidate_cols[0]\n",
    "\n",
    "    if date_col is None:\n",
    "        fallback = original_cols[0]\n",
    "        parsed = _coerce_datetime(raw[fallback].to_list())\n",
    "        if any(v is not None for v in parsed):\n",
    "            raw = raw.with_columns(pl.Series(fallback, parsed))\n",
    "            date_col = fallback\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"No date-like column detected.\")\n",
    "\n",
    "    processed = raw.with_columns(pl.Series(date_col, _coerce_datetime(raw[date_col].to_list())))\n",
    "    processed = processed.filter(pl.col(date_col).is_not_null())\n",
    "    processed = processed.sort(date_col)\n",
    "    processed = processed.unique(subset=[date_col], keep=\"first\", maintain_order=True)\n",
    "\n",
    "    numeric_cols = [c for c in [open_col, high_col, low_col, close_col, adj_close_col] if c]\n",
    "    for col in numeric_cols:\n",
    "        processed = processed.with_columns(pl.col(col).cast(pl.Float64, strict=False).alias(col))\n",
    "    if volume_col is not None:\n",
    "        processed = processed.with_columns(pl.col(volume_col).cast(pl.Float64, strict=False).alias(volume_col))\n",
    "\n",
    "    if numeric_cols:\n",
    "        processed = processed.with_columns([\n",
    "            pl.col(col).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\").alias(col)\n",
    "            for col in numeric_cols\n",
    "        ])\n",
    "    if volume_col is not None:\n",
    "        processed = processed.with_columns(\n",
    "            pl.when(pl.col(volume_col) == 0).then(None).otherwise(pl.col(volume_col)).alias(volume_col)\n",
    "        )\n",
    "        processed = processed.with_columns(\n",
    "            pl.col(volume_col).fill_null(strategy=\"forward\").fill_null(strategy=\"backward\").alias(volume_col)\n",
    "        )\n",
    "\n",
    "    if adj_close_col and close_col:\n",
    "        processed = processed.with_columns(\n",
    "            pl.when((pl.col(close_col) != 0) & pl.col(close_col).is_not_null())\n",
    "              .then(pl.col(adj_close_col) / pl.col(close_col))\n",
    "              .otherwise(None)\n",
    "              .alias(\"__adj_factor\")\n",
    "        )\n",
    "        processed = processed.with_columns(\n",
    "            pl.when(pl.col(\"__adj_factor\") > 0)\n",
    "              .then(pl.col(\"__adj_factor\"))\n",
    "              .otherwise(None)\n",
    "              .alias(\"__adj_factor\")\n",
    "        )\n",
    "        processed = processed.with_columns(\n",
    "            pl.col(\"__adj_factor\").fill_null(strategy=\"forward\").fill_null(strategy=\"backward\").alias(\"__adj_factor\")\n",
    "        )\n",
    "        if open_col:\n",
    "            processed = processed.with_columns((pl.col(open_col) * pl.col(\"__adj_factor\")).alias(\"adj_open\"))\n",
    "        if high_col:\n",
    "            processed = processed.with_columns((pl.col(high_col) * pl.col(\"__adj_factor\")).alias(\"adj_high\"))\n",
    "        if low_col:\n",
    "            processed = processed.with_columns((pl.col(low_col) * pl.col(\"__adj_factor\")).alias(\"adj_low\"))\n",
    "        processed = processed.with_columns((pl.col(close_col) * pl.col(\"__adj_factor\")).alias(\"adj_close\"))\n",
    "        processed = processed.drop(\"__adj_factor\")\n",
    "\n",
    "    if \"adj_close\" not in processed.columns:\n",
    "        fallback_close = adj_close_col or close_col\n",
    "        if fallback_close is None:\n",
    "            raise ValueError(\"No usable closing price column.\")\n",
    "        processed = processed.with_columns(pl.col(fallback_close).alias(\"adj_close\"))\n",
    "\n",
    "    processed = processed.rename({date_col: \"date\"})\n",
    "    if volume_col is not None and volume_col != \"volume\":\n",
    "        processed = processed.rename({volume_col: \"volume\"})\n",
    "        volume_col = \"volume\"\n",
    "\n",
    "    processed = processed.with_columns(pl.col(\"adj_close\").log().diff().alias(\"ret\"))\n",
    "    processed = processed.with_columns(winsorize_series(processed[\"ret\"]).rename(\"ret_w\"))\n",
    "\n",
    "    vol_20 = processed[\"ret\"].rolling_std(window_size=20, min_periods=20)\n",
    "    vol_60 = processed[\"ret\"].rolling_std(window_size=60, min_periods=60)\n",
    "    processed = processed.with_columns([\n",
    "        vol_20.rename(\"vol_20\"),\n",
    "        vol_60.rename(\"vol_60\"),\n",
    "    ])\n",
    "    processed = processed.with_columns([\n",
    "        (processed[\"vol_20\"] * math.sqrt(252.0)).rename(\"rv_20_annual\"),\n",
    "        (processed[\"vol_60\"] * math.sqrt(252.0)).rename(\"rv_60_annual\"),\n",
    "    ])\n",
    "    ret_vol_scaled = processed[\"ret\"] / processed[\"vol_20\"]\n",
    "    processed = processed.with_columns(ret_vol_scaled.rename(\"ret_vol_scaled\"))\n",
    "    processed = processed.with_columns(winsorize_series(processed[\"ret_vol_scaled\"]).rename(\"ret_vol_scaled_w\"))\n",
    "\n",
    "    adj_close_series = processed[\"adj_close\"]\n",
    "    delta = adj_close_series.diff()\n",
    "    gain_series = delta.map_elements(lambda x: x if x is not None and x > 0 else 0.0, return_dtype=pl.Float64, skip_nulls=False)\n",
    "    loss_series = delta.map_elements(lambda x: -x if x is not None and x < 0 else 0.0, return_dtype=pl.Float64, skip_nulls=False)\n",
    "    avg_gain = gain_series.ewm_mean(alpha=1.0 / 14.0, adjust=False, ignore_nulls=True, min_periods=14)\n",
    "    avg_loss = loss_series.ewm_mean(alpha=1.0 / 14.0, adjust=False, ignore_nulls=True, min_periods=14)\n",
    "    denom = avg_loss.map_elements(lambda x: None if x is None or x == 0 else x, return_dtype=pl.Float64, skip_nulls=False)\n",
    "    rs = avg_gain / denom\n",
    "    rsi = rs.map_elements(lambda x: 100.0 - (100.0 / (1.0 + x)) if x is not None else None, return_dtype=pl.Float64, skip_nulls=False)\n",
    "    processed = processed.with_columns(rsi.rename(\"rsi14\"))\n",
    "\n",
    "    ema_fast = adj_close_series.ewm_mean(alpha=2.0 / (12.0 + 1.0), adjust=False, ignore_nulls=True)\n",
    "    ema_slow = adj_close_series.ewm_mean(alpha=2.0 / (26.0 + 1.0), adjust=False, ignore_nulls=True)\n",
    "    macd_series = ema_fast - ema_slow\n",
    "    macd_signal = macd_series.ewm_mean(alpha=2.0 / (9.0 + 1.0), adjust=False, ignore_nulls=True)\n",
    "    macd_hist = macd_series - macd_signal\n",
    "    processed = processed.with_columns([\n",
    "        macd_series.rename(\"macd\"),\n",
    "        macd_signal.rename(\"macd_signal\"),\n",
    "        macd_hist.rename(\"macd_hist\"),\n",
    "    ])\n",
    "\n",
    "    bb_mid = adj_close_series.rolling_mean(window_size=20, min_periods=20)\n",
    "    bb_std = adj_close_series.rolling_std(window_size=20, min_periods=20)\n",
    "    processed = processed.with_columns([\n",
    "        bb_mid.rename(\"bb_mid\"),\n",
    "        (bb_mid + 2.0 * bb_std).rename(\"bb_upper\"),\n",
    "        (bb_mid - 2.0 * bb_std).rename(\"bb_lower\"),\n",
    "    ])\n",
    "\n",
    "    base_cols = [\"date\", \"adj_open\", \"adj_high\", \"adj_low\", \"adj_close\"]\n",
    "    tidy_cols: List[str] = [c for c in base_cols if c in processed.columns]\n",
    "    if volume_col is not None and volume_col in processed.columns:\n",
    "        tidy_cols.append(volume_col)\n",
    "    feature_cols = [\n",
    "        \"ret\", \"ret_w\", \"ret_vol_scaled\", \"ret_vol_scaled_w\",\n",
    "        \"vol_20\", \"vol_60\", \"rv_20_annual\", \"rv_60_annual\",\n",
    "        \"rsi14\", \"macd\", \"macd_signal\", \"macd_hist\",\n",
    "        \"bb_mid\", \"bb_upper\", \"bb_lower\",\n",
    "    ]\n",
    "    tidy_cols.extend([c for c in feature_cols if c in processed.columns])\n",
    "\n",
    "    tidy = processed.select(tidy_cols).sort(\"date\")\n",
    "    tidy = tidy.with_columns(pl.lit(path.stem).alias(\"series\"))\n",
    "    tidy = tidy.select([\"series\"] + [c for c in tidy.columns if c != \"series\"])\n",
    "\n",
    "    tidy = tidy.with_columns(pl.col(\"date\").cast(pl.Datetime, strict=False))\n",
    "    date_series = tidy[\"date\"]\n",
    "    non_null_dates = date_series.drop_nulls()\n",
    "    if not non_null_dates.is_empty():\n",
    "        start_dt = non_null_dates.min()\n",
    "        end_dt = non_null_dates.max()\n",
    "        start_str = start_dt.strftime(\"%Y-%m-%d\")\n",
    "        end_str = end_dt.strftime(\"%Y-%m-%d\")\n",
    "        gap_series = tidy.select(pl.col(\"date\").diff().dt.total_days()).to_series().drop_nulls()\n",
    "        median_gap = float(gap_series.median()) if not gap_series.is_empty() else math.nan\n",
    "    else:\n",
    "        start_str = None\n",
    "        end_str = None\n",
    "        median_gap = math.nan\n",
    "    feature_base = {\"series\", \"date\", \"volume\", \"adj_open\", \"adj_high\", \"adj_low\", \"adj_close\"}\n",
    "    feature_count = len([c for c in tidy.columns if c not in feature_base])\n",
    "\n",
    "    meta: Dict[str, object] = {\n",
    "        \"series\": path.stem,\n",
    "        \"raw_file\": path.name,\n",
    "        \"rows\": int(tidy.height),\n",
    "        \"start\": start_str,\n",
    "        \"end\": end_str,\n",
    "        \"median_gap_days\": median_gap,\n",
    "        \"has_volume\": \"volume\" in tidy.columns,\n",
    "        \"feature_columns\": feature_count,\n",
    "    }\n",
    "\n",
    "    return tidy, meta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7b9611",
   "metadata": {},
   "source": [
    "## 2.5 Batch Execution Across FULL Nexus Data\n",
    "\n",
    "Iterate through the Excel universe, persist preprocessed artefacts, and collect run metadata plus any issues for remediation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3acae363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all(files: Iterable[Path], overwrite: bool = False) -> Tuple[pl.DataFrame, pl.DataFrame]:\n",
    "    meta_entries: List[Dict[str, object]] = []\n",
    "    error_entries: List[Dict[str, object]] = []\n",
    "    for path in files:\n",
    "        series_id = path.stem\n",
    "        csv_path = OUTPUT_DIR / f\"{series_id}_preprocessed.csv\"\n",
    "        parquet_path = OUTPUT_DIR / f\"{series_id}_preprocessed.parquet\"\n",
    "\n",
    "        if not overwrite and csv_path.exists():\n",
    "            try:\n",
    "                cached = pl.read_csv(csv_path, try_parse_dates=True)\n",
    "                dates = [dt for dt in cached[\"date\"].to_list() if dt is not None]\n",
    "                if dates:\n",
    "                    deltas = [ (dates[i] - dates[i - 1]).days for i in range(1, len(dates)) ]\n",
    "                    median_gap = float(statistics.median(deltas)) if deltas else math.nan\n",
    "                    start_str = dates[0].strftime(\"%Y-%m-%d\")\n",
    "                    end_str = dates[-1].strftime(\"%Y-%m-%d\")\n",
    "                else:\n",
    "                    median_gap = math.nan\n",
    "                    start_str = None\n",
    "                    end_str = None\n",
    "                meta_entries.append({\n",
    "                    \"series\": series_id,\n",
    "                    \"raw_file\": path.name,\n",
    "                    \"rows\": int(cached.height),\n",
    "                    \"start\": start_str,\n",
    "                    \"end\": end_str,\n",
    "                    \"median_gap_days\": median_gap,\n",
    "                    \"has_volume\": \"volume\" in cached.columns,\n",
    "                    \"feature_columns\": len([c for c in cached.columns if c not in {\"series\", \"date\", \"volume\", \"adj_open\", \"adj_high\", \"adj_low\", \"adj_close\"}]),\n",
    "                    \"csv_path\": str(csv_path),\n",
    "                    \"parquet_path\": str(parquet_path) if parquet_path.exists() else \"\",\n",
    "                    \"status\": \"cached\",\n",
    "                })\n",
    "                continue\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        try:\n",
    "            tidy, meta = preprocess_single_series(path)\n",
    "        except Exception as exc:\n",
    "            error_entries.append({\"series\": series_id, \"raw_file\": path.name, \"error\": str(exc)})\n",
    "            continue\n",
    "\n",
    "        tidy.write_csv(csv_path)\n",
    "        parquet_written = False\n",
    "        try:\n",
    "            tidy.write_parquet(parquet_path)\n",
    "            parquet_written = True\n",
    "        except Exception:\n",
    "            parquet_written = False\n",
    "\n",
    "        meta.update({\n",
    "            \"csv_path\": str(csv_path),\n",
    "            \"parquet_path\": str(parquet_path) if parquet_written else \"\",\n",
    "            \"status\": \"processed\",\n",
    "        })\n",
    "        meta_entries.append(meta)\n",
    "\n",
    "    meta_table = pl.DataFrame(meta_entries)\n",
    "    error_table = pl.DataFrame(error_entries)\n",
    "    return meta_table, error_table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674fee79",
   "metadata": {},
   "source": [
    "## 2.6 Output Summary & Quality Checks\n",
    "\n",
    "Run the batch job (idempotent by default) and surface coverage diagnostics, volume availability, and any files requiring investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1fcf50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series processed: 182 | Errors: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (12, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>series</th><th>raw_file</th><th>rows</th><th>start</th><th>end</th><th>median_gap_days</th><th>has_volume</th><th>feature_columns</th><th>csv_path</th><th>parquet_path</th><th>status</th></tr><tr><td>str</td><td>str</td><td>i64</td><td>str</td><td>str</td><td>f64</td><td>bool</td><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>&quot;#N_A Field Not Applicable - 13…</td><td>&quot;#N_A Field Not Applicable - 13…</td><td>361</td><td>&quot;2023-04-06&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/#N_A Fie…</td><td>&quot;analysis/preprocessed/#N_A Fie…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;#N_A Field Not Applicable - 65…</td><td>&quot;#N_A Field Not Applicable - 65…</td><td>472</td><td>&quot;2022-07-21&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/#N_A Fie…</td><td>&quot;analysis/preprocessed/#N_A Fie…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;#N_A N_A - IBM CDS USD SR 5Y D…</td><td>&quot;#N_A N_A - IBM CDS USD SR 5Y D…</td><td>2512</td><td>&quot;2015-01-05&quot;</td><td>&quot;2025-09-17&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/#N_A N_A…</td><td>&quot;analysis/preprocessed/#N_A N_A…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;3 Month SOFR Opt  Dec25C    95…</td><td>&quot;3 Month SOFR Opt  Dec25C    95…</td><td>945</td><td>&quot;2021-12-13&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/3 Month …</td><td>&quot;analysis/preprocessed/3 Month …</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;3 Month SOFR Opt  Dec25P    95…</td><td>&quot;3 Month SOFR Opt  Dec25P    95…</td><td>945</td><td>&quot;2021-12-13&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/3 Month …</td><td>&quot;analysis/preprocessed/3 Month …</td><td>&quot;cached&quot;</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;AT&amp;T Inc - T US Equity Equity&quot;</td><td>&quot;AT&amp;T Inc - T US Equity Equity.…</td><td>6465</td><td>&quot;2000-01-03&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/AT&amp;T Inc…</td><td>&quot;analysis/preprocessed/AT&amp;T Inc…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;AUSTRALIAN DOLLAR 1 MO - AUD1M…</td><td>&quot;AUSTRALIAN DOLLAR 1 MO - AUD1M…</td><td>1491</td><td>&quot;2020-01-01&quot;</td><td>&quot;2025-09-17&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;AUSTRALIAN DOLLAR 3 MO - AUD3M…</td><td>&quot;AUSTRALIAN DOLLAR 3 MO - AUD3M…</td><td>1491</td><td>&quot;2020-01-01&quot;</td><td>&quot;2025-09-17&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;AUSTRALIAN DOLLAR_US DOLLAR - …</td><td>&quot;AUSTRALIAN DOLLAR_US DOLLAR - …</td><td>5403</td><td>&quot;2005-01-03&quot;</td><td>&quot;2025-09-17&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;analysis/preprocessed/AUSTRALI…</td><td>&quot;cached&quot;</td></tr><tr><td>&quot;Alibaba Group Holding Ltd - 99…</td><td>&quot;Alibaba Group Holding Ltd - 99…</td><td>1430</td><td>&quot;2019-11-25&quot;</td><td>&quot;2025-09-16&quot;</td><td>1.0</td><td>false</td><td>15</td><td>&quot;analysis/preprocessed/Alibaba …</td><td>&quot;analysis/preprocessed/Alibaba …</td><td>&quot;cached&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (12, 11)\n",
       "┌─────────────┬────────────┬──────┬────────────┬───┬────────────┬────────────┬────────────┬────────┐\n",
       "│ series      ┆ raw_file   ┆ rows ┆ start      ┆ … ┆ feature_co ┆ csv_path   ┆ parquet_pa ┆ status │\n",
       "│ ---         ┆ ---        ┆ ---  ┆ ---        ┆   ┆ lumns      ┆ ---        ┆ th         ┆ ---    │\n",
       "│ str         ┆ str        ┆ i64  ┆ str        ┆   ┆ ---        ┆ str        ┆ ---        ┆ str    │\n",
       "│             ┆            ┆      ┆            ┆   ┆ i64        ┆            ┆ str        ┆        │\n",
       "╞═════════════╪════════════╪══════╪════════════╪═══╪════════════╪════════════╪════════════╪════════╡\n",
       "│ #N_A Field  ┆ #N_A Field ┆ 361  ┆ 2023-04-06 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ Not         ┆ Not        ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Applicable  ┆ Applicable ┆      ┆            ┆   ┆            ┆ d/#N_A     ┆ d/#N_A     ┆        │\n",
       "│ - 13…       ┆ - 13…      ┆      ┆            ┆   ┆            ┆ Fie…       ┆ Fie…       ┆        │\n",
       "│ #N_A Field  ┆ #N_A Field ┆ 472  ┆ 2022-07-21 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ Not         ┆ Not        ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Applicable  ┆ Applicable ┆      ┆            ┆   ┆            ┆ d/#N_A     ┆ d/#N_A     ┆        │\n",
       "│ - 65…       ┆ - 65…      ┆      ┆            ┆   ┆            ┆ Fie…       ┆ Fie…       ┆        │\n",
       "│ #N_A N_A -  ┆ #N_A N_A - ┆ 2512 ┆ 2015-01-05 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ IBM CDS USD ┆ IBM CDS    ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ SR 5Y D…    ┆ USD SR 5Y  ┆      ┆            ┆   ┆            ┆ d/#N_A     ┆ d/#N_A     ┆        │\n",
       "│             ┆ D…         ┆      ┆            ┆   ┆            ┆ N_A…       ┆ N_A…       ┆        │\n",
       "│ 3 Month     ┆ 3 Month    ┆ 945  ┆ 2021-12-13 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ SOFR Opt    ┆ SOFR Opt   ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Dec25C      ┆ Dec25C     ┆      ┆            ┆   ┆            ┆ d/3 Month  ┆ d/3 Month  ┆        │\n",
       "│ 95…         ┆ 95…        ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "│ 3 Month     ┆ 3 Month    ┆ 945  ┆ 2021-12-13 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ SOFR Opt    ┆ SOFR Opt   ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Dec25P      ┆ Dec25P     ┆      ┆            ┆   ┆            ┆ d/3 Month  ┆ d/3 Month  ┆        │\n",
       "│ 95…         ┆ 95…        ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "│ …           ┆ …          ┆ …    ┆ …          ┆ … ┆ …          ┆ …          ┆ …          ┆ …      │\n",
       "│ AT&T Inc -  ┆ AT&T Inc - ┆ 6465 ┆ 2000-01-03 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ T US Equity ┆ T US       ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Equity      ┆ Equity     ┆      ┆            ┆   ┆            ┆ d/AT&T     ┆ d/AT&T     ┆        │\n",
       "│             ┆ Equity.…   ┆      ┆            ┆   ┆            ┆ Inc…       ┆ Inc…       ┆        │\n",
       "│ AUSTRALIAN  ┆ AUSTRALIAN ┆ 1491 ┆ 2020-01-01 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ DOLLAR 1 MO ┆ DOLLAR 1   ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ - AUD1M…    ┆ MO -       ┆      ┆            ┆   ┆            ┆ d/AUSTRALI ┆ d/AUSTRALI ┆        │\n",
       "│             ┆ AUD1M…     ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "│ AUSTRALIAN  ┆ AUSTRALIAN ┆ 1491 ┆ 2020-01-01 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ DOLLAR 3 MO ┆ DOLLAR 3   ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ - AUD3M…    ┆ MO -       ┆      ┆            ┆   ┆            ┆ d/AUSTRALI ┆ d/AUSTRALI ┆        │\n",
       "│             ┆ AUD3M…     ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "│ AUSTRALIAN  ┆ AUSTRALIAN ┆ 5403 ┆ 2005-01-03 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ DOLLAR_US   ┆ DOLLAR_US  ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ DOLLAR - …  ┆ DOLLAR - … ┆      ┆            ┆   ┆            ┆ d/AUSTRALI ┆ d/AUSTRALI ┆        │\n",
       "│             ┆            ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "│ Alibaba     ┆ Alibaba    ┆ 1430 ┆ 2019-11-25 ┆ … ┆ 15         ┆ analysis/p ┆ analysis/p ┆ cached │\n",
       "│ Group       ┆ Group      ┆      ┆            ┆   ┆            ┆ reprocesse ┆ reprocesse ┆        │\n",
       "│ Holding Ltd ┆ Holding    ┆      ┆            ┆   ┆            ┆ d/Alibaba  ┆ d/Alibaba  ┆        │\n",
       "│ - 99…       ┆ Ltd - 99…  ┆      ┆            ┆   ┆            ┆ …          ┆ …          ┆        │\n",
       "└─────────────┴────────────┴──────┴────────────┴───┴────────────┴────────────┴────────────┴────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>min_start</th><th>max_start</th><th>min_end</th><th>max_end</th></tr><tr><td>date</td><td>date</td><td>date</td><td>date</td></tr></thead><tbody><tr><td>1985-01-31</td><td>2025-08-11</td><td>2025-06-30</td><td>2025-09-30</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 4)\n",
       "┌────────────┬────────────┬────────────┬────────────┐\n",
       "│ min_start  ┆ max_start  ┆ min_end    ┆ max_end    │\n",
       "│ ---        ┆ ---        ┆ ---        ┆ ---        │\n",
       "│ date       ┆ date       ┆ date       ┆ date       │\n",
       "╞════════════╪════════════╪════════════╪════════════╡\n",
       "│ 1985-01-31 ┆ 2025-08-11 ┆ 2025-06-30 ┆ 2025-09-30 │\n",
       "└────────────┴────────────┴────────────┴────────────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/s5l6v1614q77wz_4t437rdpr0000gn/T/ipykernel_93026/1916175622.py:16: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  vol_counts = meta_table.groupby(\"has_volume\", maintain_order=True).agg(pl.len().alias(\"count\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>has_volume</th><th>count</th></tr><tr><td>bool</td><td>u32</td></tr></thead><tbody><tr><td>false</td><td>182</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌────────────┬───────┐\n",
       "│ has_volume ┆ count │\n",
       "│ ---        ┆ ---   │\n",
       "│ bool       ┆ u32   │\n",
       "╞════════════╪═══════╡\n",
       "│ false      ┆ 182   │\n",
       "└────────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/03/s5l6v1614q77wz_4t437rdpr0000gn/T/ipykernel_93026/1916175622.py:18: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  status_counts = meta_table.groupby(\"status\", maintain_order=True).agg(pl.len().alias(\"count\"))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>status</th><th>count</th></tr><tr><td>str</td><td>u32</td></tr></thead><tbody><tr><td>&quot;cached&quot;</td><td>182</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 2)\n",
       "┌────────┬───────┐\n",
       "│ status ┆ count │\n",
       "│ ---    ┆ ---   │\n",
       "│ str    ┆ u32   │\n",
       "╞════════╪═══════╡\n",
       "│ cached ┆ 182   │\n",
       "└────────┴───────┘"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meta_table, error_table = preprocess_all(excel_files, overwrite=False)\n",
    "print(f\"Series processed: {meta_table.height} | Errors: {error_table.height}\")\n",
    "if meta_table.height:\n",
    "    preview = meta_table.sort([\"status\", \"series\"]).head(12)\n",
    "    display(preview)\n",
    "    coverage = meta_table.with_columns([\n",
    "        pl.col(\"start\").str.strptime(pl.Date, strict=False).alias(\"start_date\"),\n",
    "        pl.col(\"end\").str.strptime(pl.Date, strict=False).alias(\"end_date\"),\n",
    "    ]).select([\n",
    "        pl.col(\"start_date\").min().alias(\"min_start\"),\n",
    "        pl.col(\"start_date\").max().alias(\"max_start\"),\n",
    "        pl.col(\"end_date\").min().alias(\"min_end\"),\n",
    "        pl.col(\"end_date\").max().alias(\"max_end\"),\n",
    "    ])\n",
    "    display(coverage)\n",
    "    vol_counts = meta_table.groupby(\"has_volume\", maintain_order=True).agg(pl.len().alias(\"count\"))\n",
    "    display(vol_counts)\n",
    "    status_counts = meta_table.groupby(\"status\", maintain_order=True).agg(pl.len().alias(\"count\"))\n",
    "    display(status_counts)\n",
    "if error_table.height:\n",
    "    display(error_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c0d691",
   "metadata": {},
   "source": [
    "# 3. Market Dynamics Analysis\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
