{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff802aa6",
   "metadata": {},
   "source": [
    "\n",
    "# Nexus Data Preprocessing\n",
    "\n",
    "This notebook documents the Nexus equity preprocessing workflow using a staged structure inspired by the *Advanced Data Preprocessing Techniques for Financial* playbook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6a721e",
   "metadata": {},
   "source": [
    "\n",
    "## Define: Preprocessing Blueprint\n",
    "- **Stage 1 – Data Cleaning & Alignment:** Detect schemas, repair gaps, and harmonise timestamps before any modelling.\n",
    "- **Stage 2 – Noise Reduction & Signal Extraction:** Apply smoothing, frequency-domain filters, and latent-state models to dampen noise.\n",
    "- **Stage 3 – Stationarity & Transformation:** Enforce statistical consistency via differencing, scaling, and variance-stabilising transforms.\n",
    "- **Stage 4 – Feature Engineering & Enrichment:** Generate technical, macro, and alternative signals aligned with the 2025 Nexus objectives.\n",
    "- **Stage 5 – Outlier & Anomaly Handling:** Tame extreme events with robust scaling and anomaly flags.\n",
    "- **Stage 6 – Feature Selection & Dimensionality Reduction:** Reduce redundancy and highlight predictive structure.\n",
    "- **Stage 7 – Temporal & Structural Adjustments:** Respect regime shifts, rolling dynamics, and event-driven structures.\n",
    "- **Stage 8 – Cross-Sectional vs. Time-Series Treatment:** Differentiate per-asset normalisation from panel-level adjustments.\n",
    "- **Stage 9 – Data Augmentation & Synthetic Data:** Expand scenario coverage through resampling and generative models.\n",
    "- **Stage 10 – Preprocessing Pipelines (Automation):** Assemble modular layers into a reusable ETL-to-model input flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f70e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import argparse\n",
    "from typing import List, Optional, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73b6f53",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Data Cleaning & Alignment\n",
    "**Handling Missing Data**\n",
    "- Forward/Backward fill for time-series gaps.\n",
    "- Model-based imputation (Kalman filters, EM for state-space models).\n",
    "- Multiple imputation (Bayesian approaches for macroeconomic series).\n",
    "\n",
    "**Timestamp Alignment**\n",
    "- Align heterogeneous data frequencies (e.g., daily stock prices with monthly macro indicators).\n",
    "- As-of joins for tick-by-tick vs. daily aggregates.\n",
    "\n",
    "**Corporate Actions Adjustments**\n",
    "- Price adjustment for dividends, stock splits, mergers, ticker changes.\n",
    "- Rolling adjustment factors for continuity in historical series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8817419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _first_present(cols: Sequence[str], *candidates_groups: Sequence[str]) -> Optional[str]:\n",
    "    cols_l = [c.lower().strip() for c in cols]\n",
    "    for candidates in candidates_groups:\n",
    "        for cand in candidates:\n",
    "            if cand in cols_l:\n",
    "                return cols[cols_l.index(cand)]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9fbd6",
   "metadata": {},
   "source": [
    "**_first_present** – Locates the first matching alias in the schema so downstream steps can rely on canonical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cfc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_datetime(series: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(series, errors=\"coerce\", utc=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094aa16",
   "metadata": {},
   "source": [
    "**_coerce_datetime** – Converts timestamp-like inputs into timezone-naive pandas datetimes while dropping invalid entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6140f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_fill(frame: pd.DataFrame, columns: Optional[Sequence[str]] = None) -> pd.DataFrame:\n",
    "    columns = list(columns) if columns is not None else list(frame.columns)\n",
    "    out = frame.copy()\n",
    "    out[columns] = out[columns].ffill().bfill()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a925f0",
   "metadata": {},
   "source": [
    "**forward_backward_fill** – Applies forward/backward fills to stabilise gaps across the selected columns in a time-series frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38cca15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_impute(series: pd.Series, transition_variance: float = 1e-5, observation_variance: float = 1e-2) -> pd.Series:\n",
    "    try:\n",
    "        from pykalman import KalmanFilter  # type: ignore\n",
    "    except Exception:\n",
    "        return series.interpolate(method=\"linear\").ffill().bfill()\n",
    "    obs = series.to_numpy()\n",
    "    mask = ~np.isnan(obs)\n",
    "    if mask.sum() == 0:\n",
    "        return series.copy()\n",
    "    initial_state_mean = obs[mask][0]\n",
    "    kf = KalmanFilter(transition_matrices=[1.0], observation_matrices=[1.0],\n",
    "                      transition_covariance=[[transition_variance]],\n",
    "                      observation_covariance=[[observation_variance]],\n",
    "                      initial_state_mean=initial_state_mean)\n",
    "    state_means, _ = kf.em(obs, n_iter=5).smooth(obs)\n",
    "    return pd.Series(state_means[:, 0], index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef56ab83",
   "metadata": {},
   "source": [
    "**kalman_impute** – Uses a 1D Kalman smoother (with linear interpolation fallback) to estimate missing observations in noisy economic series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d181c724",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_imputation_draws(df: pd.DataFrame, n_draws: int = 5, random_state: Optional[int] = None) -> List[pd.DataFrame]:\n",
    "    if n_draws <= 0:\n",
    "        return []\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    stds = df.select_dtypes(include=[np.number]).std().replace(0.0, np.nan)\n",
    "    numeric_cols = stds.index.tolist()\n",
    "    draws: List[pd.DataFrame] = []\n",
    "    for _ in range(n_draws):\n",
    "        draw = df.copy()\n",
    "        if numeric_cols:\n",
    "            scale = np.nan_to_num(stds.to_numpy(), nan=0.0)\n",
    "            noise = rng.normal(loc=0.0, scale=scale, size=(len(df), len(numeric_cols)))\n",
    "            noise_df = pd.DataFrame(noise, index=df.index, columns=numeric_cols)\n",
    "            draw.loc[:, numeric_cols] = draw.loc[:, numeric_cols] + noise_df\n",
    "            draw.loc[:, numeric_cols] = draw.loc[:, numeric_cols].ffill().bfill()\n",
    "        draws.append(draw.ffill().bfill())\n",
    "    return draws"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41257aa9",
   "metadata": {},
   "source": [
    "**multiple_imputation_draws** – Generates stochastic imputations by injecting noise around observed values to approximate Bayesian multiple imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa897f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_frequency(df: pd.DataFrame, date_col: str, freq: str, method: str = \"ffill\") -> pd.DataFrame:\n",
    "    out = df.set_index(date_col).sort_index()\n",
    "    if method == \"ffill\":\n",
    "        out = out.resample(freq).ffill()\n",
    "    elif method == \"bfill\":\n",
    "        out = out.resample(freq).bfill()\n",
    "    else:\n",
    "        out = out.resample(freq).interpolate(method=method)\n",
    "    return out.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438720b2",
   "metadata": {},
   "source": [
    "**align_to_frequency** – Resamples a time series onto a target frequency using configurable forward/backward/interpolated fills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11402b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def asof_join(left: pd.DataFrame, right: pd.DataFrame, on: str, suffix: str = \"_rhs\",\n",
    "              tolerance: Optional[pd.Timedelta] = None, direction: str = \"backward\") -> pd.DataFrame:\n",
    "    merged = pd.merge_asof(left.sort_values(on), right.sort_values(on), on=on,\n",
    "                            suffixes=(\"\", suffix), tolerance=tolerance, direction=direction)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610b6cf",
   "metadata": {},
   "source": [
    "**asof_join** – Performs an as-of join so high-frequency series can align with lower-frequency macro indicators without look-ahead bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bcffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_ohlc_with_adjclose(df: pd.DataFrame,\n",
    "                              col_open: Optional[str],\n",
    "                              col_high: Optional[str],\n",
    "                              col_low: Optional[str],\n",
    "                              col_close: Optional[str],\n",
    "                              col_adjclose: Optional[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    if col_adjclose is None or col_close is None:\n",
    "        return out\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        factor = out[col_adjclose] / out[col_close]\n",
    "    factor = factor.replace([np.inf, -np.inf], np.nan)\n",
    "    factor = factor.where(factor > 0, np.nan).ffill().bfill()\n",
    "    if col_open is not None:\n",
    "        out[\"adj_open\"] = out[col_open] * factor\n",
    "    if col_high is not None:\n",
    "        out[\"adj_high\"] = out[col_high] * factor\n",
    "    if col_low is not None:\n",
    "        out[\"adj_low\"] = out[col_low] * factor\n",
    "    out[\"adj_close\"] = out[col_close] * factor\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5104e7",
   "metadata": {},
   "source": [
    "**adjust_ohlc_with_adjclose** – Constructs split/dividend-adjusted OHLC stacks using the adjusted-close factor to maintain historical continuity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52104816",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Noise Reduction & Signal Extraction\n",
    "**Smoothing Filters**\n",
    "- Moving average, Savitzky–Golay filters for trend preservation.\n",
    "\n",
    "**Fourier & Wavelet Transforms**\n",
    "- Multi-resolution decomposition for denoising price series.\n",
    "\n",
    "**Empirical Mode Decomposition (EMD)**\n",
    "- Decompose non-linear/non-stationary signals into Intrinsic Mode Functions.\n",
    "\n",
    "**Kalman Filtering**\n",
    "- Real-time noise reduction and latent state estimation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9912090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average_filter(series: pd.Series, window: int = 5) -> pd.Series:\n",
    "    return series.rolling(window=window, min_periods=1, center=False).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28be2063",
   "metadata": {},
   "source": [
    "**moving_average_filter** – Applies a simple moving average to emphasise medium-term trends in noisy signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c09d460",
   "metadata": {},
   "outputs": [],
   "source": [
    "def savitzky_golay_filter(series: pd.Series, window_length: int = 7, polyorder: int = 3) -> pd.Series:\n",
    "    min_length = polyorder + 2\n",
    "    if min_length % 2 == 0:\n",
    "        min_length += 1\n",
    "    window_length = max(window_length, min_length)\n",
    "    if window_length % 2 == 0:\n",
    "        window_length += 1\n",
    "    try:\n",
    "        from scipy.signal import savgol_filter  # type: ignore\n",
    "    except Exception:\n",
    "        return series.rolling(window=window_length, min_periods=1, center=True).mean()\n",
    "    filtered = savgol_filter(series.to_numpy(), window_length=window_length, polyorder=polyorder, mode=\"interp\")\n",
    "    return pd.Series(filtered, index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b540244",
   "metadata": {},
   "source": [
    "**savitzky_golay_filter** – Uses a polynomial smoothing filter (with rolling-mean fallback) to retain trend curvature while denoising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d8b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fourier_lowpass_filter(series: pd.Series, cutoff_ratio: float = 0.1) -> pd.Series:\n",
    "    values = series.to_numpy()\n",
    "    n = values.shape[0]\n",
    "    if n == 0:\n",
    "        return series.copy()\n",
    "    fft = np.fft.rfft(values)\n",
    "    cutoff = int(len(fft) * cutoff_ratio)\n",
    "    filtered_fft = np.zeros_like(fft)\n",
    "    filtered_fft[:max(cutoff, 1)] = fft[:max(cutoff, 1)]\n",
    "    filtered = np.fft.irfft(filtered_fft, n=n)\n",
    "    return pd.Series(filtered, index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac01a9aa",
   "metadata": {},
   "source": [
    "**fourier_lowpass_filter** – Keeps only the low-frequency Fourier coefficients to suppress high-frequency noise in price series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e67757a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wavelet_denoise(series: pd.Series, wavelet: str = \"db4\", level: int = 2) -> pd.Series:\n",
    "    try:\n",
    "        import pywt  # type: ignore\n",
    "    except Exception:\n",
    "        return series.copy()\n",
    "    coeffs = pywt.wavedec(series.to_numpy(), wavelet, mode=\"smooth\")\n",
    "    sigma = np.median(np.abs(coeffs[-level])) / 0.6745 if level <= len(coeffs) - 1 else 0.0\n",
    "    threshold = sigma * np.sqrt(2 * np.log(len(series))) if sigma > 0 else 0.0\n",
    "    denoised_coeffs = [coeffs[0]]\n",
    "    for c in coeffs[1:]:\n",
    "        denoised_coeffs.append(pywt.threshold(c, threshold, mode=\"soft\"))\n",
    "    reconstructed = pywt.waverec(denoised_coeffs, wavelet, mode=\"smooth\")\n",
    "    return pd.Series(reconstructed[: len(series)], index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb83c7",
   "metadata": {},
   "source": [
    "**wavelet_denoise** – Performs wavelet shrinkage (with identity fallback) to remove transient spikes while keeping structure across scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def empirical_mode_decompose(series: pd.Series) -> List[pd.Series]:\n",
    "    try:\n",
    "        from PyEMD import EMD  # type: ignore\n",
    "    except Exception:\n",
    "        return [series.copy()]\n",
    "    emd = EMD()\n",
    "    imfs = emd(series.to_numpy())\n",
    "    return [pd.Series(imf, index=series.index) for imf in imfs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25f038",
   "metadata": {},
   "source": [
    "**empirical_mode_decompose** – Decomposes non-linear signals into intrinsic mode functions when the PyEMD library is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0870b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_smoother(series: pd.Series, transition_variance: float = 1e-5, observation_variance: float = 1e-2) -> pd.Series:\n",
    "    try:\n",
    "        from pykalman import KalmanFilter  # type: ignore\n",
    "    except Exception:\n",
    "        return series.ewm(alpha=0.2, adjust=False).mean()\n",
    "    obs = series.to_numpy()\n",
    "    mask = ~np.isnan(obs)\n",
    "    if mask.sum() == 0:\n",
    "        return series.copy()\n",
    "    initial_state_mean = obs[mask][0]\n",
    "    kf = KalmanFilter(transition_matrices=[1.0], observation_matrices=[1.0],\n",
    "                      transition_covariance=[[transition_variance]],\n",
    "                      observation_covariance=[[observation_variance]],\n",
    "                      initial_state_mean=initial_state_mean)\n",
    "    state_means, _ = kf.em(obs, n_iter=5).smooth(obs)\n",
    "    return pd.Series(state_means[:, 0], index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af0a6e",
   "metadata": {},
   "source": [
    "**kalman_smoother** – Applies a Kalman smoother (with exponential-weighted fallback) for real-time noise reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecae3ed0",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Stationarity & Transformation\n",
    "**Detrending & Differencing**\n",
    "- Log returns (instead of raw prices).\n",
    "- Seasonal decomposition for macroeconomic data.\n",
    "\n",
    "**Normalization & Scaling**\n",
    "- Volatility scaling (returns divided by realized volatility).\n",
    "- Z-score scaling for features with different magnitudes (e.g., rates vs. sentiment).\n",
    "\n",
    "**Box-Cox / Yeo-Johnson Transforms**\n",
    "- Variance-stabilizing transformations for skewed economic data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe94d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_returns(series: pd.Series) -> pd.Series:\n",
    "    return np.log(series).diff()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f03f58",
   "metadata": {},
   "source": [
    "**compute_log_returns** – Transforms price levels into log returns to enforce stationarity in financial time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc73985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_difference(series: pd.Series, period: int) -> pd.Series:\n",
    "    return series.diff(periods=period)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ad1bcb",
   "metadata": {},
   "source": [
    "**seasonal_difference** – Applies seasonal differencing to remove periodic structure from macroeconomic indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5601ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_adjust(series: pd.Series, period: int = 12) -> pd.Series:\n",
    "    try:\n",
    "        from statsmodels.tsa.seasonal import STL  # type: ignore\n",
    "    except Exception:\n",
    "        trend = series.rolling(window=period, min_periods=1).mean()\n",
    "        return series - trend\n",
    "    stl = STL(series, period=period, robust=True)\n",
    "    result = stl.fit()\n",
    "    return series - result.seasonal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b53351",
   "metadata": {},
   "source": [
    "**seasonal_adjust** – Removes seasonal components using STL (with rolling-mean fallback) to stabilise macro features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442e340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def volatility_scale(returns: pd.Series, vol: pd.Series) -> pd.Series:\n",
    "    return returns / vol.replace(0.0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7f0641",
   "metadata": {},
   "source": [
    "**volatility_scale** – Divides returns by realised volatility to normalise distributions across market regimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a6f6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize(series: pd.Series) -> pd.Series:\n",
    "    mean = series.mean()\n",
    "    std = series.std()\n",
    "    if std == 0:\n",
    "        return series - mean\n",
    "    return (series - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d460d6a4",
   "metadata": {},
   "source": [
    "**zscore_normalize** – Standardises a feature to zero mean and unit variance for cross-feature comparability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dc95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boxcox_transform(series: pd.Series, lmbda: Optional[float] = None) -> pd.Series:\n",
    "    shifted = series + 1 - series.min()\n",
    "    try:\n",
    "        from scipy import stats  # type: ignore\n",
    "    except Exception:\n",
    "        return np.log1p(shifted)\n",
    "    transformed, _ = stats.boxcox(shifted, lmbda=lmbda)\n",
    "    return pd.Series(transformed, index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdc08fb",
   "metadata": {},
   "source": [
    "**boxcox_transform** – Applies Box-Cox (with log fallback) to stabilise variance for strictly positive series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cdf3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yeojohnson_transform(series: pd.Series, lmbda: Optional[float] = None) -> pd.Series:\n",
    "    try:\n",
    "        from scipy import stats  # type: ignore\n",
    "    except Exception:\n",
    "        return zscore_normalize(series)\n",
    "    transformed, _ = stats.yeojohnson(series, lmbda=lmbda)\n",
    "    return pd.Series(transformed, index=series.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7f4ab4",
   "metadata": {},
   "source": [
    "**yeojohnson_transform** – Uses Yeo-Johnson (with z-score fallback) to stabilise skewed series that include negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e50e9e",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Feature Engineering & Enrichment\n",
    "**Market Features**\n",
    "- Volatility clustering (e.g., GARCH residuals as features).\n",
    "- Higher-order moments (skewness, kurtosis of returns).\n",
    "- Technical indicators (RSI, MACD, Bollinger Bands).\n",
    "\n",
    "**Macroeconomic Features**\n",
    "- Lagged effects (e.g., interest rates, CPI lags).\n",
    "- Cyclical indicators (yield curve slope, credit spreads).\n",
    "\n",
    "**Alternative Data Integration**\n",
    "- Social sentiment (NLP preprocessing: embeddings, topic modeling).\n",
    "- Satellite, ESG, shipping flows, Google Trends (scaled to market calendars).\n",
    "\n",
    "**Market Regimes**\n",
    "- Hidden Markov Models (HMM) or Bayesian Change Point Detection to label bull/bear/stagnant phases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235e54e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rsi(prices: pd.Series, window: int = 14) -> pd.Series:\n",
    "    delta = prices.diff()\n",
    "    gain = delta.clip(lower=0).ewm(alpha=1 / window, adjust=False, min_periods=window).mean()\n",
    "    loss = -delta.clip(upper=0).ewm(alpha=1 / window, adjust=False, min_periods=window).mean()\n",
    "    rs = gain / loss.replace(0, np.nan)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3bee1a",
   "metadata": {},
   "source": [
    "**compute_rsi** – Computes the Relative Strength Index to capture short-horizon momentum in adjusted closing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab4be96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_macd(prices: pd.Series, span_fast: int = 12, span_slow: int = 26, span_signal: int = 9) -> pd.DataFrame:\n",
    "    ema_fast = prices.ewm(span=span_fast, adjust=False).mean()\n",
    "    ema_slow = prices.ewm(span=span_slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal = macd_line.ewm(span=span_signal, adjust=False).mean()\n",
    "    hist = macd_line - signal\n",
    "    return pd.DataFrame({\"macd\": macd_line, \"macd_signal\": signal, \"macd_hist\": hist})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9566f",
   "metadata": {},
   "source": [
    "**compute_macd** – Generates MACD, signal, and histogram series to summarise trend shifts via dual exponential averages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe332b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bbands(prices: pd.Series, window: int = 20, n_std: float = 2.0) -> pd.DataFrame:\n",
    "    ma = prices.rolling(window=window, min_periods=window).mean()\n",
    "    sd = prices.rolling(window=window, min_periods=window).std()\n",
    "    upper = ma + n_std * sd\n",
    "    lower = ma - n_std * sd\n",
    "    return pd.DataFrame({\"bb_mid\": ma, \"bb_upper\": upper, \"bb_lower\": lower})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38010a4",
   "metadata": {},
   "source": [
    "**compute_bbands** – Builds Bollinger Bands around a rolling mean to contextualise prices against recent volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_garch_residuals(returns: pd.Series, p: int = 1, q: int = 1) -> Optional[pd.Series]:\n",
    "    try:\n",
    "        from arch import arch_model  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    clean = returns.dropna()\n",
    "    if clean.empty:\n",
    "        return None\n",
    "    model = arch_model(clean, vol=\"Garch\", p=p, q=q, dist=\"normal\")\n",
    "    fit = model.fit(disp=\"off\")\n",
    "    resid = fit.resid / fit.conditional_volatility\n",
    "    return resid.reindex(returns.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10da7ec5",
   "metadata": {},
   "source": [
    "**compute_garch_residuals** – Fits a GARCH model (when `arch` is available) to extract volatility-clustered residual signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aac1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_higher_moments(returns: pd.Series, window: int = 60) -> pd.DataFrame:\n",
    "    skew = returns.rolling(window=window, min_periods=window).skew()\n",
    "    kurt = returns.rolling(window=window, min_periods=window).kurt()\n",
    "    return pd.DataFrame({\"rolling_skew\": skew, \"rolling_kurt\": kurt})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1564bcbe",
   "metadata": {},
   "source": [
    "**rolling_higher_moments** – Computes rolling skewness and kurtosis to quantify distributional shape changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d99403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_macro_lags(df: pd.DataFrame, date_col: str, columns: Sequence[str], lags: Sequence[int]) -> pd.DataFrame:\n",
    "    out = df.sort_values(date_col).set_index(date_col)\n",
    "    for col in columns:\n",
    "        for lag in lags:\n",
    "            out[f\"{col}_lag{lag}\"] = out[col].shift(lag)\n",
    "    return out.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f302d92",
   "metadata": {},
   "source": [
    "**build_macro_lags** – Generates lagged macroeconomic predictors to capture delayed market reactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3568196e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_alternative_signals(price_df: pd.DataFrame, alt_df: pd.DataFrame, date_col: str,\n",
    "                                  tolerance: Optional[str] = \"1D\") -> pd.DataFrame:\n",
    "    tol_td = pd.to_timedelta(tolerance) if tolerance is not None else None\n",
    "    merged = asof_join(price_df, alt_df, on=date_col, suffix=\"_alt\", tolerance=tol_td)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b478e919",
   "metadata": {},
   "source": [
    "**integrate_alternative_signals** – Adds alternative datasets (sentiment, ESG, etc.) via tolerant as-of joins aligned to the market calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8893a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_market_regimes(series: pd.Series, fast_window: int = 20, slow_window: int = 100) -> pd.Series:\n",
    "    fast_ma = series.rolling(window=fast_window, min_periods=1).mean()\n",
    "    slow_ma = series.rolling(window=slow_window, min_periods=1).mean()\n",
    "    regime = pd.Series(\"stagnant\", index=series.index)\n",
    "    regime = regime.mask(fast_ma > slow_ma, \"bull\")\n",
    "    regime = regime.mask(fast_ma < slow_ma, \"bear\")\n",
    "    return regime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33916ce0",
   "metadata": {},
   "source": [
    "**detect_market_regimes** – Labels simple bull/bear/stagnant regimes using dual moving-average differentiation (HMM-ready placeholder)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9575f",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Outlier & Anomaly Handling\n",
    "**Winsorization / Clipping**\n",
    "- Reduce impact of extreme tail events.\n",
    "\n",
    "**Robust Scaling**\n",
    "- Median & interquartile-based scaling (instead of mean/variance).\n",
    "\n",
    "**Anomaly Detection**\n",
    "- Isolation forests, robust PCA, or autoencoders for unusual price/volume/macro shocks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daebf4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def winsorize(series: pd.Series, lower_q: float = 0.01, upper_q: float = 0.99) -> pd.Series:\n",
    "    lo, hi = series.quantile([lower_q, upper_q])\n",
    "    return series.clip(lower=lo, upper=hi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235087f",
   "metadata": {},
   "source": [
    "**winsorize** – Clips series tails at configurable quantiles to reduce outsized influence from market shocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dbcc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def robust_scale(series: pd.Series) -> pd.Series:\n",
    "    median = series.median()\n",
    "    iqr = series.quantile(0.75) - series.quantile(0.25)\n",
    "    if iqr == 0:\n",
    "        return series - median\n",
    "    return (series - median) / iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda25c3",
   "metadata": {},
   "source": [
    "**robust_scale** – Normalises a feature using median and IQR for resilience to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a252df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_anomalies_zscore(series: pd.Series, z_thresh: float = 3.0) -> pd.Series:\n",
    "    z = zscore_normalize(series)\n",
    "    return (np.abs(z) > z_thresh).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26542ad0",
   "metadata": {},
   "source": [
    "**detect_anomalies_zscore** – Flags anomalies via absolute z-score thresholds as a lightweight alternative to heavier models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe406697",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Feature Selection & Dimensionality Reduction\n",
    "**Filter & Wrapper Methods**\n",
    "- Mutual information, stability selection for economic variables.\n",
    "\n",
    "**PCA / ICA / RPCA**\n",
    "- Remove redundancy from correlated indicators.\n",
    "\n",
    "**Manifold Learning**\n",
    "- t-SNE, UMAP for clustering latent regimes.\n",
    "\n",
    "**Sparse Methods**\n",
    "- Lasso/ElasticNet to select predictive macro factors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b149b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_ranking(features: pd.DataFrame, target: pd.Series) -> pd.Series:\n",
    "    try:\n",
    "        from sklearn.feature_selection import mutual_info_regression  # type: ignore\n",
    "    except Exception:\n",
    "        corr = features.corrwith(target)\n",
    "        return corr.abs().sort_values(ascending=False)\n",
    "    mi = mutual_info_regression(features.fillna(0.0), target.fillna(0.0))\n",
    "    return pd.Series(mi, index=features.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9312c970",
   "metadata": {},
   "source": [
    "**mutual_information_ranking** – Scores features by mutual information (or correlation fallback) to prioritise informative signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85525652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def principal_component_features(features: pd.DataFrame, n_components: int = 3) -> pd.DataFrame:\n",
    "    centered = features - features.mean()\n",
    "    cov = np.cov(centered.fillna(0.0).to_numpy().T)\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = np.argsort(eigvals)[::-1][:n_components]\n",
    "    pcs = centered.to_numpy() @ eigvecs[:, order]\n",
    "    columns = [f\"pc{i+1}\" for i in range(len(order))]\n",
    "    return pd.DataFrame(pcs, index=features.index, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8d375",
   "metadata": {},
   "source": [
    "**principal_component_features** – Produces principal components via eigen-decomposition to summarise correlated inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f728260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def independent_component_features(features: pd.DataFrame, n_components: int = 3) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        from sklearn.decomposition import FastICA  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    ica = FastICA(n_components=n_components, random_state=42, whiten=\"unit-variance\")\n",
    "    comps = ica.fit_transform(features.fillna(0.0))\n",
    "    columns = [f\"ica{i+1}\" for i in range(comps.shape[1])]\n",
    "    return pd.DataFrame(comps, index=features.index, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49e87bb",
   "metadata": {},
   "source": [
    "**independent_component_features** – Runs FastICA (if available) to uncover statistically independent drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3345ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_feature_selection(features: pd.DataFrame, target: pd.Series, alpha: float = 0.001) -> Optional[pd.Series]:\n",
    "    try:\n",
    "        from sklearn.linear_model import Lasso  # type: ignore\n",
    "    except Exception:\n",
    "        return None\n",
    "    model = Lasso(alpha=alpha, max_iter=5000)\n",
    "    model.fit(features.fillna(0.0), target.fillna(0.0))\n",
    "    return pd.Series(model.coef_, index=features.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c57c613",
   "metadata": {},
   "source": [
    "**sparse_feature_selection** – Applies Lasso (when scikit-learn is installed) to highlight sparse predictive macro factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95676da3",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Temporal & Structural Adjustments\n",
    "**Regime-Specific Preprocessing**\n",
    "- Normalize/scale separately within regimes (bull vs. bear).\n",
    "\n",
    "**Time-Varying Correlations**\n",
    "- Dynamic Conditional Correlation (DCC-GARCH).\n",
    "\n",
    "**Rolling Window Features**\n",
    "- Adaptive statistics (mean/volatility/correlation over rolling periods).\n",
    "\n",
    "**Event-Time Alignment**\n",
    "- Align around earnings announcements, FOMC dates, recessions, policy changes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbb81eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regime_specific_normalize(series: pd.Series, regimes: pd.Series) -> pd.Series:\n",
    "    out = series.copy()\n",
    "    for regime, mask in regimes.groupby(regimes).groups.items():\n",
    "        out.iloc[list(mask)] = zscore_normalize(series.iloc[list(mask)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0457c6",
   "metadata": {},
   "source": [
    "**regime_specific_normalize** – Applies z-score scaling within each detected regime to respect structural breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4a2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_correlation(series_a: pd.Series, series_b: pd.Series, window: int = 60) -> pd.Series:\n",
    "    return series_a.rolling(window=window, min_periods=window).corr(series_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586f7edf",
   "metadata": {},
   "source": [
    "**dynamic_correlation** – Tracks rolling correlation to proxy time-varying dependence (DCC-ready placeholder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac7cbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_features(series: pd.Series, window: int = 20) -> pd.DataFrame:\n",
    "    roll_mean = series.rolling(window=window, min_periods=window).mean()\n",
    "    roll_std = series.rolling(window=window, min_periods=window).std()\n",
    "    return pd.DataFrame({\"rolling_mean\": roll_mean, \"rolling_std\": roll_std})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808315a",
   "metadata": {},
   "source": [
    "**rolling_window_features** – Generates rolling mean and volatility as adaptive statistics for downstream scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53012717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_to_events(df: pd.DataFrame, date_col: str, events: Sequence[pd.Timestamp], window: int = 5) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    df = df.sort_values(date_col)\n",
    "    for event in events:\n",
    "        mask = (df[date_col] >= event - pd.Timedelta(days=window)) & (df[date_col] <= event + pd.Timedelta(days=window))\n",
    "        window_df = df.loc[mask].copy()\n",
    "        window_df[\"event_date\"] = event\n",
    "        window_df[\"t_minus_event\"] = (window_df[date_col] - event).dt.days\n",
    "        frames.append(window_df)\n",
    "    if not frames:\n",
    "        return pd.DataFrame(columns=df.columns.tolist() + [\"event_date\", \"t_minus_event\"])\n",
    "    return pd.concat(frames, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e35870",
   "metadata": {},
   "source": [
    "**align_to_events** – Builds event-time panels around key dates (earnings, policy) for structural analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf11c31",
   "metadata": {},
   "source": [
    "\n",
    "## 8. Cross-Sectional vs. Time-Series Treatment\n",
    "**Cross-Sectional Normalization**\n",
    "- Rank or z-score standardization across assets each day.\n",
    "\n",
    "**Panel Data Handling**\n",
    "- Fixed effects (sector/country dummies).\n",
    "- Random effects for heterogeneous asset panels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3389d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_sectional_rank_normalize(df: pd.DataFrame, date_col: str, value_col: str) -> pd.Series:\n",
    "    ranks = df.groupby(date_col)[value_col].rank(method=\"average\")\n",
    "    group_sizes = df.groupby(date_col)[value_col].transform(\"count\")\n",
    "    return (ranks - 0.5) / group_sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fed944",
   "metadata": {},
   "source": [
    "**cross_sectional_rank_normalize** – Produces daily fractional ranks for cross-sectional standardisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc2ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_fixed_effects_encode(df: pd.DataFrame, categories: Sequence[str]) -> pd.DataFrame:\n",
    "    out = df.copy()\n",
    "    for cat in categories:\n",
    "        dummies = pd.get_dummies(out[cat], prefix=cat, drop_first=True)\n",
    "        out = pd.concat([out, dummies], axis=1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb04d90c",
   "metadata": {},
   "source": [
    "**panel_fixed_effects_encode** – Adds fixed-effect dummies (e.g., sector, country) to absorb persistent panel biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66ab12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_random_effects_placeholder():\n",
    "    raise NotImplementedError(\"Random effects estimators require specialised econometrics libraries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdda2d",
   "metadata": {},
   "source": [
    "**panel_random_effects_placeholder** – Signals where to plug in a random-effects estimator when econometrics tooling is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea329c38",
   "metadata": {},
   "source": [
    "\n",
    "## 9. Data Augmentation & Synthetic Data\n",
    "**Bootstrapping & Block Bootstraps**\n",
    "- Preserve autocorrelation in returns for resampling.\n",
    "\n",
    "**Generative Models**\n",
    "- GANs, VAEs for synthetic return/macro series.\n",
    "\n",
    "**Backtesting Augmentation**\n",
    "- Scenario generation (stress events, fat-tail shocks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0339473a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_bootstrap(series: pd.Series, block_size: int = 5, n_samples: int = 100, random_state: Optional[int] = None) -> np.ndarray:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    values = series.dropna().to_numpy()\n",
    "    if len(values) == 0:\n",
    "        return np.empty((n_samples, 0))\n",
    "    blocks = [values[i:i + block_size] for i in range(0, len(values), block_size)]\n",
    "    bootstrapped = np.array([\n",
    "        np.concatenate(rng.choice(blocks, size=max(len(blocks), 1), replace=True))[:len(values)]\n",
    "        for _ in range(n_samples)\n",
    "    ])\n",
    "    return bootstrapped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54c0fd6",
   "metadata": {},
   "source": [
    "**block_bootstrap** – Samples contiguous blocks to retain autocorrelation when generating synthetic return paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7d2bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_scenarios(mean: pd.Series, cov: pd.DataFrame, n_scenarios: int = 100, random_state: Optional[int] = None) -> pd.DataFrame:\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    draws = rng.multivariate_normal(mean.fillna(0.0), cov.fillna(0.0), size=n_scenarios)\n",
    "    return pd.DataFrame(draws, columns=mean.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e47520",
   "metadata": {},
   "source": [
    "**generate_synthetic_scenarios** – Creates Gaussian scenarios from estimated mean/covariance as a lightweight stand-in for GAN/VAEs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af430bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_event_shocks(series: pd.Series, shock_factor: float = 3.0) -> pd.Series:\n",
    "    shocks = series.copy()\n",
    "    tail_mask = detect_anomalies_zscore(series, z_thresh=2.0).astype(bool)\n",
    "    shocks.loc[tail_mask] = shocks.loc[tail_mask] * shock_factor\n",
    "    return shocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b828515",
   "metadata": {},
   "source": [
    "**stress_event_shocks** – Amplifies detected tail events to craft stress scenarios for backtesting robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd5200c",
   "metadata": {},
   "source": [
    "\n",
    "## 10. Preprocessing Pipelines (Automation)\n",
    "**Modular pipelines with:**\n",
    "- ETL Layer: Raw data ingestion & corporate actions adjustment.\n",
    "- Preprocessing Layer: Cleaning, scaling, alignment.\n",
    "- Feature Layer: Market/macroeconomic/alternative features.\n",
    "- Regime Layer: State-dependent preprocessing.\n",
    "- Model Input Layer: Final normalized dataset for ML/RL agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25697ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_preprocessing_pipeline(df: pd.DataFrame,\n",
    "                                 date_col: str,\n",
    "                                 open_col: Optional[str],\n",
    "                                 high_col: Optional[str],\n",
    "                                 low_col: Optional[str],\n",
    "                                 close_col: Optional[str],\n",
    "                                 adj_close_col: Optional[str],\n",
    "                                 volume_col: Optional[str]) -> pd.DataFrame:\n",
    "    df = df.sort_values(date_col).drop_duplicates(subset=[date_col]).reset_index(drop=True)\n",
    "    numeric_cols = [c for c in [open_col, high_col, low_col, close_col, adj_close_col, volume_col] if c is not None]\n",
    "    for col in numeric_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    df[numeric_cols] = df[numeric_cols].ffill().bfill()\n",
    "    df = adjust_ohlc_with_adjclose(df, open_col, high_col, low_col, close_col, adj_close_col)\n",
    "    if volume_col is not None:\n",
    "        df.loc[df[volume_col] == 0, volume_col] = np.nan\n",
    "        df[volume_col] = df[volume_col].ffill().bfill()\n",
    "    if \"adj_close\" not in df.columns:\n",
    "        if close_col is None:\n",
    "            raise ValueError(\"No usable close or adjusted close column.\")\n",
    "        df[\"adj_close\"] = df[close_col]\n",
    "    df[\"ret\"] = compute_log_returns(df[\"adj_close\"])\n",
    "    df[\"ret_w\"] = winsorize(df[\"ret\"])\n",
    "    df[\"vol_20\"] = df[\"ret\"].rolling(window=20, min_periods=20).std()\n",
    "    df[\"vol_60\"] = df[\"ret\"].rolling(window=60, min_periods=60).std()\n",
    "    df[\"rv_20_annual\"] = df[\"vol_20\"] * np.sqrt(252.0)\n",
    "    df[\"rv_60_annual\"] = df[\"vol_60\"] * np.sqrt(252.0)\n",
    "    df[\"ret_vol_scaled\"] = volatility_scale(df[\"ret\"], df[\"vol_20\"])\n",
    "    df[\"ret_vol_scaled_w\"] = winsorize(df[\"ret_vol_scaled\"])\n",
    "    rsi14 = compute_rsi(df[\"adj_close\"], window=14)\n",
    "    macd_df = compute_macd(df[\"adj_close\"], span_fast=12, span_slow=26, span_signal=9)\n",
    "    bb_df = compute_bbands(df[\"adj_close\"], window=20, n_std=2.0)\n",
    "    df = pd.concat([df, rsi14.rename(\"rsi14\"), macd_df, bb_df], axis=1)\n",
    "    higher_moments = rolling_higher_moments(df[\"ret\"], window=60)\n",
    "    df = pd.concat([df, higher_moments], axis=1)\n",
    "    regimes = detect_market_regimes(df[\"adj_close\"], fast_window=20, slow_window=100)\n",
    "    df[\"regime\"] = regimes\n",
    "    df[\"ret_regime_scaled\"] = regime_specific_normalize(df[\"ret\"], regimes)\n",
    "    df[\"anomaly_flag\"] = detect_anomalies_zscore(df[\"ret_vol_scaled_w\"], z_thresh=3.0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf9d942",
   "metadata": {},
   "source": [
    "**build_preprocessing_pipeline** – Chains the cleaning, feature, regime, and anomaly steps into a single reusable data frame transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de39d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_cat_excel(input_path: str, output_base: str) -> Tuple[str, Optional[str]]:\n",
    "    try:\n",
    "        df = pd.read_excel(input_path)\n",
    "    except Exception:\n",
    "        df = pd.read_excel(input_path, engine=\"openpyxl\")\n",
    "    original_cols = list(df.columns)\n",
    "    date_col = _first_present(original_cols, [\"date\", \"timestamp\", \"time\", \"datetime\"])\n",
    "    open_col = _first_present(original_cols, [\"open\", \"px_open\", \"o\"])\n",
    "    high_col = _first_present(original_cols, [\"high\", \"px_high\", \"h\"])\n",
    "    low_col = _first_present(original_cols, [\"low\", \"px_low\", \"l\"])\n",
    "    close_col = _first_present(original_cols, [\"close\", \"px_last\", \"price\", \"last\"])\n",
    "    adj_close_col = _first_present(original_cols, [\"adj close\", \"adjusted close\"])\n",
    "    if adj_close_col is not None:\n",
    "        close_col = close_col or _first_present(original_cols, [\"close\", \"px_last\", \"price\", \"last\"])\n",
    "    volume_col = _first_present(original_cols, [\"volume\", \"vol\", \"qty\", \"turnover\"])\n",
    "    if date_col is None:\n",
    "        maybe_date = original_cols[0]\n",
    "        parsed = pd.to_datetime(df[maybe_date], errors=\"coerce\")\n",
    "        if parsed.notna().mean() > 0.7:\n",
    "            date_col = maybe_date\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"No date/time column detected.\")\n",
    "    df[date_col] = _coerce_datetime(df[date_col])\n",
    "    df = df.dropna(subset=[date_col])\n",
    "    processed = build_preprocessing_pipeline(df, date_col, open_col, high_col, low_col, close_col, adj_close_col, volume_col)\n",
    "    adj_cols_available = [c for c in [\"adj_open\", \"adj_high\", \"adj_low\"] if c in processed.columns]\n",
    "    out_cols = [date_col] + adj_cols_available + [\"adj_close\"]\n",
    "    if volume_col is not None:\n",
    "        out_cols.append(volume_col)\n",
    "    out_cols += [\"ret\", \"ret_w\", \"ret_vol_scaled\", \"ret_vol_scaled_w\",\n",
    "                 \"vol_20\", \"vol_60\", \"rv_20_annual\", \"rv_60_annual\",\n",
    "                 \"rsi14\", \"macd\", \"macd_signal\", \"macd_hist\",\n",
    "                 \"bb_mid\", \"bb_upper\", \"bb_lower\",\n",
    "                 \"rolling_skew\", \"rolling_kurt\",\n",
    "                 \"regime\", \"ret_regime_scaled\", \"anomaly_flag\"]\n",
    "    tidy = processed[out_cols].copy()\n",
    "    rename_map = {date_col: \"date\"}\n",
    "    if volume_col is not None:\n",
    "        rename_map[volume_col] = \"volume\"\n",
    "    tidy = tidy.rename(columns=rename_map)\n",
    "    tidy[\"date\"] = pd.to_datetime(tidy[\"date\"]).dt.tz_localize(None)\n",
    "    csv_path = f\"{output_base}.csv\"\n",
    "    tidy.to_csv(csv_path, index=False)\n",
    "    parquet_path: Optional[str] = None\n",
    "    try:\n",
    "        import pyarrow  # type: ignore\n",
    "        parquet_path = f\"{output_base}.parquet\"\n",
    "        tidy.to_parquet(parquet_path, index=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return csv_path, parquet_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c169736e",
   "metadata": {},
   "source": [
    "**preprocess_cat_excel** – Executes the Nexus pipeline end-to-end: ingest Excel, harmonise schema, engineer features, and persist artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da168cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    parser = argparse.ArgumentParser(description=\"Preprocess equity time series (CAT US.xlsx style).\")\n",
    "    parser.add_argument(\"--input\", required=True, help=\"Path to Excel file\")\n",
    "    parser.add_argument(\"--output_base\", required=True, help=\"Output path prefix (no extension)\")\n",
    "    args = parser.parse_args()\n",
    "    csv_path, pq_path = preprocess_cat_excel(args.input, args.output_base)\n",
    "    print(\"Wrote:\", csv_path)\n",
    "    if pq_path:\n",
    "        print(\"Wrote:\", pq_path)\n",
    "    else:\n",
    "        print(\"Parquet not written (pyarrow missing).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d3f617",
   "metadata": {},
   "source": [
    "**main** – CLI entrypoint that wires command-line arguments into the preprocessing pipeline, reporting output artefacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfc1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
